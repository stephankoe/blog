---
title: Parallelisierung des Maschinellen Lernens
description: Dieser Artikel beschreibt verschiedene Ans√§tze zur Parallelisierung von Trainings-Algorithmen im Maschinellen Lernen.
date: 2024-01-28 09:04:28+0800
image: cover.jpg
toc: true
math: true
draft: true
categories:
   - distributed-computing
   - machine-learning
weight: 1
---

# Parallelisierung des Maschinellen Lernens

J√ºngste Forschungen √ºber skalierbare Architekturen wie den Transformer [@vaswaniAttentionAllYou2017] haben gezeigt, dass die Leistungsf√§higkeit von neuronalen Netzen mit der Anzahl ihrer Parameter korreliert [@kaplanScalingLawsNeural2020] und dabei mit besonders gro√üen Netzwerken beeindruckende Ergebnisse erzielt [@openaiIntroducingChatGPT2022; @openaiGPT4TechnicalReport2023]. Dies f√ºhrte j√ºngst zu einem Wettlauf um die gr√∂√üten Modelle und damit verbunden einem drastischen Anstieg der Anforderungen an die Trainings- und Inferenz-Hardware. Diese Architekturen lassen sich zwar quasi beliebig vergr√∂√üern, doch sind der Skalierung in der Praxis durch die Hardware Grenzen gesetzt [@narayananEfficientLargescaleLanguage2021]. So hat sich beispielsweise die Zahl der Parameter der gr√∂√üten Modelle von 2018 bis 2021 vertausendfacht, w√§hrend die Speicherkapazit√§t der popul√§rsten Beschleuniger, Nvidia-Grafikkarten, sich in demselben Zeitraum lediglich verdoppelt hat. Mit der Gr√∂√üe der neuronalen Netze steigt auch die Trainingszeit und die Anzahl der f√ºr das Training ben√∂tigten Daten stetig an. Um also das Training immer gr√∂√üerer Modelle zu erm√∂glichen, ist die Verteilung der Rechenleistung auf mehrere Rechenknoten unumg√§nglich.

![Ein Liniendiagramm, das die Gr√∂√üe der neuronalen Netze in den Jahren von 2018 bis 2021 darstellt. Es sind die folgenden KI-Modelle eingezeichnet: 2018: ELMo (94 Mio.), BERT-L (340 Mio.); 2019: GPT-2 (1,5 Mrd.), Megatron-LM (8,3 Mrd.); 2020: Turing-NLG (17,2 Mrd.), GPT-3 (175 Mrd.). Alle diese Modelle liegen auf einer Geraden in einem logarithmischen Ma√üstab.](img/neural-network-size.png)
: Gr√∂√üenentwicklung von neuronalen Netzen √ºber die Zeit. Im Zeitraum von 2018 bis 2021 ist die Anzahl der Parameter in modernen neuronalen Netzen exponentiell angestiegen. Quelle: [@narayananEfficientLargescaleLanguage2021].

Dieser Artikel soll eine Einf√ºhrung in das verteilte Rechnen mit dem Fokus auf das maschinellen Lernen bieten. Zun√§chst werden die Grundlagen erl√§utert, anschlie√üend werden mit Daten- und Modellparallelisierung sowie Mixture of Experts verschiedene Formen der Parallelisierung eingef√ºhrt, die in der j√ºngsten Literatur diskutiert werden und in der Praxis bereits rege Anwendung finden.

## Grundlagen

Das Training immer gr√∂√üerer neuronaler Netze stellt die Entwickler vor zwei grunds√§tzliche Probleme: 

1. die f√ºr das Training n√∂tige Rechenzeit steigt mit der Anzahl der zu trainierenden Parameter an,
2. der Speicher aktueller Grafikkarten ist zu klein f√ºr die gro√üen Modelle inklusive ihrer Aktivierungen und Gradienten.

Diese Probleme k√∂nnen jeweils durch die Verteilung des Trainings auf mehrere Rechenknoten gel√∂st werden. Das Training von Modellen der Gr√∂√üe von ChatGPT und GPT-4 wird durch Parallelisierung √ºberhaupt erst m√∂glich gemacht. Doch f√ºhrt diese Parallelisierung zu einem weiteren Problem:

3. der durch Kommunikation und Synchronisation der Rechenknoten entstehende Mehraufwand steigt mit der Anzahl der Rechenknoten an.

### Scale Out Vs. Scale Up üöß

- Scale Up nur begrenzt m√∂glich, scale out erlaubt in der Theorie beliebige Skalierungsfaktoren (siehe [unten](#skalierbarkeit)).
- Effizienzsteigerung eine Form von Scle UP?

### Skalierbarkeit

- Ahmdahl: strong scalingüöß
- Gustavson: weak scalingüöß

Soll durch eine Parallelisierung die Rechenzeit verk√ºrzt werden, ist eine Betrachtung des Skalierungsverhaltens des Programms vonn√∂ten. Nicht jedes Programm profitiert gleicherma√üen von einer Verteilung auf mehrere Prozesse. Die Beschleunigung $S_p$ (engl.: *Speedup*), die durch die Verteilung auf $p$ Prozesse erreicht wird, ist das Verh√§ltnis zwischen der Rechenzeit[^Latenz] eines einzelnen Prozesses $T_1$ zur Rechenzeit von $p$ Prozessen $T_p$:

$$
S_p = \frac{T_1}{T_p}
$$
Auf Basis der Beschleunigung kann die Effizienz $E_p$ als das Verh√§ltnis der Beschleunigung $S_p$ zur Anzahl der Prozesse $p$ berechnet werden: 

$$
E_p = \frac{S_p}{p} = \frac{T_1}{T_p \cdot p}
$$
Die Effizienz (engl.: *efficiency*) gibt an, inwiefern das betrachtete Programm durch die Verteilung auf $n$ Prozesse profitiert. Bei einer Effizienz von $E_p = 1$ skaliert das Programm *linear*, also eine Erh√∂hung der Rechenkerne um einen Faktor $f$ f√ºhrt zu einer Erh√∂hung der Beschleunigung um den gleichen Faktor. Eine Effizienz von $E_p < 1$ deutet auf ein sub-lineares Skalierungsverhalten hin und eine Effizienz von $E_p > 1$ auf eine super-lineare Skalierung. Das folgende Schaubild verdeutlicht diese Beziehung:

![Speedup](img/scalability.svg)
: Verh√§ltnis der Beschleunigung (Speedup, $S_P$) zur Anzahl der parallelen Prozesse (\#Workers, $P$): Entspricht die Beschleunigung der Erh√∂hung der parallelen Prozesse ($S_P = P$, blaue Linie), skaliert das Programm linear; ist die Beschleunigung gr√∂√üer als die Anzahl der Prozesse ($S_P > P$, gr√ºner Bereich), liegt eine super-lineare Skalierung vor und im umgekehrten Fall ($S_P < P$, roter Bereich) skaliert das Programm sub-linear.

Zwar gibt es einige Algorithmen, die eine linear oder sogar super-linear skalieren [@aklSuperlinearPerformanceRealTime2004;@ristovSuperlinearSpeedupHPC2016], doch ist in der Praxis eine sub-lineare Skalierung aufgrund durch die Parallelisierung zus√§tzlich anfallender Aufgaben wie Kommunikation und Synchronisation zwischen den Prozessen die Regel [@mccoolStructuredParallelPrograming2012].

In der Forschung zur Parallelisierung von neuronalen Netzen wird h√§ufig nicht die Rechenzeit als Basis zur Berechnung der Beschleunigung herangezogen, da sich aufgrund der i.d.R. notwendigen Gradientenakkumulierung eine super-lineare Beschleunigung nur schwer erreichen l√§sst. Beispielsweise berechnen [@rajbhandariZeROMemoryOptimizations2020] die Beschleunigung in Bezug auf die Anzahl der Flie√ükommazahloperationen pro Sekunde (FLOPs).  üöß

[^Latenz]: Der Fachbegriff f√ºr die Rechenzeit ist "Latenz" (engl.: *latency*). 

### Klassifikation von Rechnerarchitekturenüöß

Flynn's taxonomy:

- SISD
- MISD
- SPMD/SIMD/SIMT
- MIMD/MPMD

### Rechnerhardware und Infrastruktur

Matrix-Operationen bilden die Grundlage f√ºr das Rechnen mit neuronalen Netzen. Diese lassen sich i.d.R. effizient parallelisieren, wodurch GPUs aufgrund ihrer tausenden von spezialisierten Prozessoren geeigneter f√ºr das Rechnen mit neuronalen Netzen sind als CPUs. Moderne Rechencluster setzen daher haupts√§chlich auf GPUs oder speziell f√ºr die Anwendung im maschinellen Lernen entwickelte Neural Processing Units (NPUs). Beispiele f√ºr NPUs sind Googles TPU [@jouppiInDatacenterPerformanceAnalysis2017a; @googleSystemArchitectureCloud2023] und Huaweis Ascend-Plattform [@huaweiAscendComputing2024].

Da der Speicher einzelner GPUs begrenzt ist, aber die Gr√∂√üe moderner neuronaler Netze exponentiell anw√§chst [@narayananEfficientLargescaleLanguage2021], kommen in der k√ºnstlichen Intelligenz vermehrt verteilte Architekturen zum Einsatz.

Ein auf GPUs basierendes Cluster besteht aus mehreren Racks mit mittels Ethernet oder InfiniBand √ºber Netzwerkswitches verbundenen Rechnern, an welchen wiederum √ºber PCIe oder herstellerspezifische Sockets wie SXM mehrere GPUs angeschlossen sind. Die GPUs wiederum k√∂nnen mittels bandbreitenstarker Direktverbindungen und spezialisierter Switches zus√§tzlich untereinander vernetzt werden, um h√∂here Bandbreiten bei √úbertragungen zwischen den GPUs eines Rechners zu erm√∂glichen[^NVLink]. Die Verbindungen in den unteren Schichten dieses Baumes haben i.d.R. die h√∂chste Bandbreite, w√§hrend in den dar√ºber liegenden Schichten die Bandbreite weiter abnimmt.

![Topologie eines GPU-Clusters: Vier Server sind √ºber einen Switch mittels d√ºnner Linien verbunden, unterhalb der Server sind jeweils vier GPUs abgebildet, die mit dem Server √ºber dickere Linien verbunden sind. Zus√§tzlich sind die vier GPUs jedes Servers √ºber sehr dicke Linien untereinander vernetzt.](img/gpu-cluster-topology.svg)
: Die Netzwerktopologie eines GPU-Clusters. Die Dicke der Verbindungslinien zwischen den Komponenten ist proportional zu deren Bandbreite.

- SuperPOD üöß (xPod?) https://docs.nvidia.com/https://docs.nvidia.com/dgx-superpod-reference-architecture-dgx-h100.pdf

Im Gegensatz zur hierarchischen Topologie in GPU-Clustern sind TPUs in einer 2D-Matrix angeordnet. Jede TPU ist selbst netzwerkf√§hig und √ºber schnelle Direktverbindungen mit jeweils vier anderen TPUs verbunden. Insgesamt bilden die TPUs einen Torus, was u.a. effiziente All-Reduce-Operationen erm√∂glicht [@jouppiDomainspecificSupercomputerTraining2020].

![Topologie eines TPU-Clusters: Die TPUs sind in einer 2D-Matrix angeordnet und sind horizontal und vertikal miteinander verbunden. Die TPUs an den R√§ndern der Matrix sind mit den jeweiligen TPUs an den anderen Enden verbunden, sodass die Struktur insgesamt einen Torus bildet.](img/tpu-cluster-topology.svg)
: Die Netzwerktopologie eines TPU-Clusters. Die Farben dienen lediglich der besseren Unterscheidung der Verbindungen.

[^NVLink]: NVIDIA bietet diesbez√ºglich NVLink und NVSwitch an [@nvidiaNVLinkNVSwitchFastest2023].

## Ans√§tze

Inspiriert durch die Arbeit von @zhengAlpaAutomatingInter2022 verwende ich in diesem Artikel eine ganzheitliche Sicht auf die Parallelisierung im maschinellen Lernen. Jedes ML-Modell kann in Form eines Graphs dargestellt werden, dessen Knoten entweder Daten (Eingabedaten, Parameter oder Aktivierungen) oder Operationen (Addition, Multiplikation, ...) darstellen. Bei der Verteilung eines Modells werden Teile des Graphs entlang einer oder mehrerer Dimensionen partitioniert und verschiedene Ger√§te verteilt. Die Operationen werden schlie√ülich auf allen Ger√§ten ausgef√ºhrt, auf denen ein Teil der Eingabedaten liegt.

![Es sind vier Graphen zu sehen.](img/parallelism-overview.svg)

: √úbersicht √ºber verschiedene g√§ngige Formen der Parallelisierung: Ein Rechengraph (a) kann auf verschiedene Arten verteilt werden. Bei der Datenparallelisierung (b) werden die Daten auf mehrere Knoten verteilt, bei der Tensor-Parallelisierung (c) die Modellparameter und bei der Pipeline-Parallelisierung (d) der Graph selbst. Gegebenenfalls m√ºssen zus√§tzlich Kommunikationsoperationen durchgef√ºhrt werden.

In der Literatur wird in Abh√§ngigkeit von den partitionierten Daten in Datenparallelisierung, Tensor-Parallelisierung und Pipeline-Parallelisierung unterschieden. Bei der Datenparallelisierung werden die Eingabedaten sowie Aktivierungen auf mehrere Ger√§te verteilt, bei der Tensor-Parallelisierung hingegen die Modellparameter und Aktivierungen, w√§hrend bei der Pipeline-Parallelisierung ganze Teile des Graphs verteilt werden. Diese Formen der Parallelisierung werden im Folgenden n√§her erl√§utert. Obwohl sich diese Formen der Parallelisierung gleicherma√üen auf die Inference anwenden lassen, liegt der Fokus in den folgenden Ausf√ºhrungen auf das Training, da sie sich bei der Vorw√§rtsberechnung nicht unterscheiden.

- welche Ans√§tze erm√∂glichen strong/weak scaling?üöß

## Datenparallelisierung

Die Datenparallelislierung ist die einfachste und am weitesten verbreitete Form der Parallelisierung im maschinellen Lernen. Beim der Datenparallelisierung werden die Eingabedaten auf mehrere Rechenknoten verteilt, was das Training auf gr√∂√üeren Datens√§tzen erm√∂glicht[^spmd]. Jeder Rechenknoten beh√§lt dabei das vollst√§ndige Modell im Speicher, wodurch die Gr√∂√üe des Modells weiterhin vom verf√ºgbaren Speicher abh√§ngt [@rajbhandariZeROMemoryOptimizations2020]. Jeder Knoten berechnet unabh√§ngig voneinander die Ausgabe und Gradienten und synchronisiert anschlie√üend die lokalen Gradienten mit den √ºbrigen Knoten mittels einer All-Reduce-Operation. Abschlie√üend aktualisiert jeder Knoten seine lokalen Modellparameter auf Basis der akkumulierten Gradienten. In der folgenden Graphik werden die Eingabedaten entlang der Batch-Dimension auf die verschiedenen Rechenknoten verteilt. Jeder Rechenknoten berechnet anschlie√üend unabh√§ngig voneinander die Ausgabe.

[^spmd]: Datenparallelisierung kann auch als "[Single Program Multiple Data](https://de.wikipedia.org/wiki/Single-Program_Multiple-Data)" (SPMD) betrachtet werden, wobei im maschinellen Lernen das KI-Modell mit seinen Instruktionen und Modellparametern das ausgef√ºhrte Programm darstellt.

![SVG image](img/data-parallel.svg)
: Datenparalleles Training von neuronalen Netzen. Das Modell wird auf $n$ Knoten repliziert, w√§hrend die Trainingsdaten gleichm√§√üig auf die Rechenknoten verteilt werden. Hier werden die Daten entlang der Batch-Dimension verteilt.

√úblicherweise werden die Daten entlang der Batch-Dimension verteilt, doch k√∂nnen die Daten auch entlang der anderen Dimensionen (z.B. der Sequenzl√§nge) partitioniert werden. In einigen Dom√§nen wie z.B. der nat√ºrlichen Sprachverarbeitung k√∂nnen die Eingabedaten sehr lange Sequenzen enthalten, wie z.B. lange Dokumente oder Audiodaten, was bei einigen Modellarchitekturen zu Speicherproblemen f√ºhrt. So hat beispielsweise der Transformer [@vaswaniAttentionAllYou2017] eine quadratische Zeit- und Speicherkomplexit√§t in Bezug auf die Eingabel√§nge [@childGeneratingLongSequences2019]. @liSequenceParallelismLong2022 f√ºhren daher die Sequenzparallelisierung ein, bei der die Sprachdaten anhand der L√§ngendimension partitioniert werden. Hierdurch wird das Training auf Datens√§tzen mit z.T. langen Eingabedokumenten erm√∂glicht. Es sind jedoch Anpassungen f√ºr Modellarchitekturen notwendig, die das gesamte Dokument betrachten. Im Falle des Transformers, der die √Ñhnlichkeit jedes Tokens im Eingabedokument mit jedem anderen Token berechnet, schlagen die Autoren *Ring Self Attention* vor, eine Variante des Aufmerksamkeitsmechanismus [@vaswaniAttentionAllYou2017], bei dem die partitionierten Key- und Value-Embeddings zwischen allen Rechenknoten zirkulieren.

Da beim datenparallelen Training alle Rechenknoten alle Modellparameter mitsamt der zugeh√∂rigen Gradienten und Optimierer-Zust√§nden speichern m√ºssen, ist die Modellgr√∂√üe durch den verf√ºgbaren Speicher der einzelnen Knoten begrenzt. @rajbhandariZeROMemoryOptimizations2020 stellen deshalb den *Zero Redundancy Optimizer* (ZeRO) vor, der durch die gleichm√§√üige Verteilung der Optimierer-Zust√§nde, Gradienten und Modellparameter auf alle Rechenknoten den Speicherverbrauch drastisch reduziert. Das Kommunikationsvolumen bleibt bei der Partitionierung der Optimierer-Zust√§nde und Gradienten im Vergleich zum datenparallelen Training unver√§ndert, wohingegen es sich bei der Partitionierung der Modellparameter um 50% erh√∂ht[^zeropp]. Die Autoren weisen in Experimenten nach, dass das datenparallele Training mit diesem Optimierer in Bezug auf FLOPs super-linear skaliert.

[^zeropp]: In der Praxis kann diese Erh√∂hung des Kommunikationsvolumens einen nicht-vernachl√§ssigbaren Einfluss auf den Trainingsdurchsatz haben. Daher kombinieren @wangZeROExtremelyEfficient2023 Quantisierung mit einer neuen Platzierungsstrategie, um den Overhead durch die Kommunikation zu reduzieren.

PyTorch unterst√ºtzt mit PyTorch DDP [@liPyTorchDistributedExperiences2020a] schon seit l√§ngerem datenparalleles Training nativ. Die Unterst√ºtzung f√ºr die zus√§tzliche Partitionierung von Optimierer-Zust√§nden, Gradienten und Modellparametern wurde mit PyTorch FSDP [@zhaoPyTorchFSDPExperiences2023] in PyTorch 2.0 eingef√ºhrt.

- Parameter-Server [@liScalingDistributedMachine2014] (Skalierungsproblem) üöß
- DeepSpeed Ulysses: Sequenzparallelisierung @jacobsDeepSpeedUlyssesSystem2023 

## Modellparallelisierung

Bei der Modellparallelisierung werden die Parameter des Modells auf verschiedene Rechenknoten verteilt. Es wird hierbei zwischen Tensor- und Pipeline-Parallelisierung unterschieden.

### Pipeline-Parallelisierung

Bei der Pipeline-Parallelisierung (PP) [@huangGPipeEfficientTraining2019; @narayananPipeDreamGeneralizedPipeline2019; @fanDAPPLEPipelinedData2020] wird das Modell in Abschnitte aufeinander folgender Operatoren zerteilt und diese Abschnitte verschiedenen Rechenknoten zugeordnet. Die Ergebnisse der einzelnen Abschnitte werden mittels Punkt-zu-Punkt-Kommunikation[^sendrecv] auf den Rechenknoten mit dem jeweils folgenden Abschnitt √ºbertragen. H√§ufig wird das Modell entlang der Schichtengrenzen zerteilt, da das Kommunikationsvolumen hier √ºblicherweise gering ist.

[^sendrecv]: Die zugeh√∂rigen Kommunikationsprimitive werden *Send* (senden) und *Recv* (receive, empfangen) genannt.

Diese Form der Parallelisierung erm√∂glicht prinzipiell ein beliebiges Skalieren der Modelltiefe. Der Vorteil dabei ist das bei geeigneter Wahl der Abschnitte geringe Kommunikationsvolumen und damit auch f√ºr Netzwerke mit geringerer Bandbreite geeignet. Allerdings f√ºhrt die Hintereinanderausf√ºhrung der Abschnitte zu Phasen geringer Rechenlast, die sog. *bubble time*, bei denen nur ein Teil der reservierten Rechenknoten genutzt wird. Ein Hauptziel der Optimierung von pipeline-parallelen Setups ist daher die Minimierung der Bubble-Time.

Durch die Verwendung von Micro-Batches [@huangGPipeEfficientTraining2019] kann die Bubble-Zeit verringert werden. Dabei werden die herk√∂mmlichen Mini-Batches weiter zerteilt und nacheinander in das Modell eingegeben. Die geringere Berechnungszeit von kleineren Batches und die √ºberlappende Bearbeitung der Mini-Batches durch verschiedene Rechenknoten f√ºhrt zu einer geringeren Latenz und damit zu einer k√ºrzeren Idle-Zeit der Rechenknoten. Sobald die Gradienten aller Micro-Batches vorliegen, werden die Parameter auf allen Rechenknoten gleichzeitig aktualisiert.

![](img/pp-microbatches.svg)
: Visualisierung der Ausf√ºhrung eines Modells (links) mittels Pipeline-Parallelisierung auf vier Beschleunigern. Der Mini-Batch wurde in acht Micro-Batches unterteilt, welche jeweils nacheinander in das Modell eingegeben werden. Nachdem die R√ºckw√§rtsberechnung des achten Micro-Batches beendet wurde, werden die Parameter auf Basis der berechneten Gradienten aktualisiert und die n√§chsten Micro-Batches in das Modell eingegeben.

- Einf√ºhrung: Schedule
- PipeDream 1F1B [@narayananPipeDreamGeneralizedPipeline2019]: Neuordnung der Verarbeitungsreihenfolge
- Reduzierung der aktiven Micro-Batches
- Geringerer Speicherverbrauch: Weniger Aktivierungen m√ºssen im Speicher vorgehalten werden, $p$
Anhand eines geeigneten Schedules kann die Bubble-Zeit weiter verringert werden, indem Rechen- und Synchronisationsaufgaben gleichzeitig ausgef√ºhrt ausgef√ºhrt werden. Beispielsweise f√ºhrt in PipeDream-1F1B[^1f1b] [@narayananPipeDreamGeneralizedPipeline2019] jeder Knoten abwechselnd eine Vorw√§rts- und R√ºckw√§rtsberechnung aus, sobald der erste Micro-Batch alle Rechenknoten passiert hat. Der Vorteil von 1F1B ist der geringere Speicheranforderungen. Beim naiven Schedule m√ºssen Aktivierungen f√ºr alle Micro-Batches gleichzeitig vorgehalten werden[^activations], w√§hrend beim 1F1B lediglich maximal $p$ Micro-Batches gleichzeitig aktiv sind, wobei $p$ die Anzahl der Modell-Partitionen bezeichnet.

[^1f1b]: 1F1B steht f√ºr "*one forward one backward*"
[^activations]: Die Anzahl der n√∂tigen Aktivierungen kann zwar mittels *Activation Checkpointing* reduziert werden, doch m√ºssen die Eingabe-Aktivierungen jeder Modell-Partition weiterhin gespeichert werden. Daher bleibt der Speicheraufwand proportional zur Anzahl der Micro-Batches.

![](img/pp-1f1b.svg)

- Ziel: Reduzierung der Bubble-Zeit um den Faktor $v$ (Anzahl der Modell-Partitionen pro Knoten)

Eine Optimierung davon ist der verschachtelte (*interleaved*) 1F1B-Schedule [@narayananEfficientLargescaleLanguage2021], bei dem ein Rechenknoten f√ºr mehrere nicht aufeinander folgende Partitionen des Modells zust√§ndig ist (*model chunks*) und die Micro-Batches dementsprechend kleiner gew√§hlt werden.

![](img/pp-1f1b-interleaved.svg)

@liChimeraEfficientlyTraining2021 kombinieren die Pipeline-Parallelisierung mit Datenparallelisierung auf Ebene der Micro-Batches. Dabei werden die einzelnen Modellschichten auf $n$ Rechenknoten repliziert, sodass gleichzeitig $n$ Berechnungen stattfinden k√∂nnen. Damit entstehen mehrere logische Pipelines auf der gleichen Menge von Knoten, wodurch eine bessere Auslastung erreicht wird.

Bubble-Zeit entsteht in der Regel am Beginn und am Ende der Berechnung eines Mini-Batches, da am Ende der Berechnung die Parameter auf allen Knoten gleichzeitig aktualisiert werden m√ºssen. Im Gegensatz zu dieser synchronen Form der Pipeline-Parallelisierung werden bei der asynchronen Pipeline-Parallelisierung [@narayananMemoryEfficientPipelineParallelDNN2021] die Modellparameter bereits nach X aktualisiert. Der asynchrone Schedule h√§lt die doppelte Anzahl der Parameter im Speicher, da die Parameter vor und nach der Aktualisierung ben√∂tigt werden. Diese Variante der Pipeline-Parallelisierung ist allerdings nicht mathematisch √§quivalent zur synchronen Variante und Konvergenz ist nicht garantiert.



- Asynchrone PP: Parameter-Updates zu unterschiedlichen Zeitpunkten (2 Versionen des Modells zur gleichen Zeit), Konvergenz nicht garantiert, nicht mathematisch √§quivalent zur synchronen Variante, z.B. Pipe-Dream-2BW [@narayananMemoryEfficientPipelineParallelDNN2021]
- Chimera: Inter-P scheme whose DP-degree is 2 [@liangSurveyAutoParallelismLargeScale2023]
- Token-level parallelism: makes good use of the property of the transformer that long sequences require a longer time to compute [@liangSurveyAutoParallelismLargeScale2023]. Instead of feeding data in the unit of micro-batch to the pipeline, terapipe [@liTeraPipeTokenLevelPipeline2021] splits sequence data along the token axis (i.e., length axis) unevenly and then feeds them in the pipeline where each spit has a similar execution time, orthogonal to TP, helpful in large-scale language models
- if the cut point is found properly, the comm is small, but the synch version will inevitably lead to bubble time
- Similar to a relay race (Staffellauf)
- Zero Bubble Pipeline Parallelism
- More activations in first nodes (more concurrent micro batches)

### Tensor-Parallelisierung

Bei der Tensor-Parallelisierung werden die Parameter-Tensoren des Modells partitioniert und auf die verschiedenen Rechenknoten verteilt. Gegebenenfalls muss auf die Operation eine Reduktion folgen, um das abschlie√üende Ergebnis zu erhalten. Beispielsweise liegt bei einer verteilten Matrixmultiplikation das Ergebnis erst nach einem All-Reduce vollst√§ndig vor.

Megatron-LM [@shoeybiMegatronLMTrainingMultiBillion2020] partitioniert die Parameter der Matrixmultiplikationen im Self-Attention-Block sowie der MLPs spaltenweise. Diese Matrizen ben√∂tigen vor allem bei Transformern sehr viel Speicher (quadratisch in der Sequenzl√§nge), sodass deren Verteilung auf verschiedene Rechenknoten sehr viel Speicher einsparen kann. Allerdings ist durch die nachfolgende Synchronisation das Kommunikationsvolumen bei dieser Form der Parallelisierung sehr hoch, weshalb sie hohe Bandbreiten zwischen den Rechenknoten verlangt, auf denen die Operation ausgef√ºhrt wird. Aufgrund des hierarchischen Aufbaus von GPU-Clustern mit bandbreitenstarken Direktverbindungen zwischen den einzelnen GPUs eines einzelnen Rechners und den vergleichsweise langsamen Verbindungen zwischen verschiedenen Rechnern, ist diese Form der Parallelisierung √ºblicherweise am geeignetsten f√ºr die Anwendung zwischen den GPUs eines einzelnen Rechners, deren Bandbreite sie voll ausnutzen kann [@shoeybiMegatronLMTrainingMultiBillion2020].

Grafik: Matrixmultiplikation üöß

Neben den eindimensionalen spaltenweisen Partitionierungen der Matrixmultiplikationen in Megatron-LM wurden auch andere Arten der Partitionierung vorgestellt, wie z.B. die reihenweise Partitionierung üöß oder die mehrdimensionale Partitionierung (2D, 2,5D, 3D) [@xuEfficient2DMethod2021; @bianMaximizingParallelismDistributed2021; @liColossalAIUnifiedDeep2022]. Obwohl sich bei der mehrdimensionalen Partitionierung in der Summe das Kommunikationsvolumen erh√∂ht, skaliert sie besser als die eindimensionale Partitionierung, da bei der eindimensionalen Partitionierung ein All-Reduce √ºber alle Knoten vonn√∂ten ist, w√§hrend sich bei der h√∂her-dimensionalen Partitionierung durch mehrere voneinander unabh√§ngige All-Reduce-Operationen eine geringere Latenz erreichen l√§sst üöß.

Grafik: nd-Partitionierung üöß

@korthikantiReducingActivationRecomputation2022 schlagen neben der Partitionierung der Matrizen der Matrixmultiplikationen des Transformers zus√§tzlich die Parallelisierung der LayerNorm- und Dropout-Operatoren vor. Obwohl die Parameter dieser Operatoren im Vergleich zu den Parametern des Aufmerksamkeitsmechanismus und MLPs relativ klein sind, kann durch deren Verteilung der f√ºr die Aktivierungen n√∂tiger Speicher merklich reduziert werden. Da im Gegensatz zu den oben genannten Matrixoperationen diese Operationen reihenweise partitioniert werden üöß, ist ein zus√§tzlicher Umformungsschritt n√∂tig, um die Daten an den Schnittstellen zum Aufmerksamkeits- und MLP-Modul korrekt umzuwandeln (üöß welche Primitive?). Sie nennen diese Form der Parallelisierung Sequenzparallelisierung, doch hat diese Methode au√üer dem Namen keinerlei Gemeinsamkeiten mit der von @liSequenceParallelismLong2022 vorgestellten Sequenzparallelisierung. In der Praxis hat sich die Kombination von spaltenweiser Tensor-Parallelisierung mit Sequenzparallelisierung bew√§hrt, da sie zueinander komplement√§r sind und der geringere Speicherverbrauch entweder gr√∂√üere Batch-Gr√∂√üen erm√∂glicht, die wiederum den Durchsatz erh√∂hen, oder das Training √ºberhaupt erst m√∂glich machen. Zudem l√§sst sich der durch die zus√§tzlichen Umformungsschritte entstehende Overhead durch geschickte Verschmelzung mit den nachfolgenden Operationen deutlich reduzieren.üöß

- make full use of bandwidth, communication brought by segmentation is basically efficient collective commüöß
- SUMMA @vandegeijnSUMMAScalableUniversal1997, 3D matmul @agarwalThreedimensionalApproachParallel1995

## Hybride Parallelisierung

- 3D CNN hybrid parallelism [@oyamaCaseStrongScaling2021 üöß

## Expertenparallelisierung üöß

Engl.: Expert parallelism, mixture of experts
- @huangHarderTasksNeed2024

## Automatische Parallelisierung

Das manuelle Design von optimalen Parallelisierungsstrategien kann vor allem bei inhomogenen Modellarchitekturen und Rechenclustern zeit- und arbeitsaufw√§ndig sein und erfordert √ºberdies viel Expertenwissen. Dies ist vor allem bei sich ver√§ndernden Gegebenheiten oder bei der Verwendung verschieden strukturierter Rechencluster schwer umsetzbar. Daher haben bereits verschiedene Arbeiten Algorithmen zur automatisierten Herleitung von Parallelisierungsstrategien vorgestellt [@jiaDataModelParallelism2018; @wangSupportingVeryLarge2019; @zhengAlpaAutomatingInter2022; @liuColossalAutoUnifiedAutomation2023; @wangImprovingAutomaticParallel2023].  

@jiaDataModelParallelism2018 @wangSupportingVeryLarge2019

Besonders hervorzuheben ist hierbei die Arbeit von @zhengAlpaAutomatingInter2022, welche mit Alpa ein System vorstellen, das auf Basis des statischen Rechengraphs eines Modells sowie der tats√§chlichen physischen Gegebenheiten des Rechenclusters eine nahezu optimale Parallelisierungsstrategie unter Einbezug von Inter-Operator- und Intra-Operator-Parallelisierung berechnet. Hierbei bezeichnet Inter-Operator-Parallelisierung das Zerteilen des Rechengraphs entlang der Kanten, w√§hrend bei der Intra-Operator-Parallelisierung die Operatoren, bzw. die Knoten, des Graphs selbst partitioniert werden. Inter-Operator-Parallelisierung ist damit eine Verallgemeinerung der [Pipeline-Parallelisierung](#pipeline-parallelisierung). Intra-Operator-Parallelisierung hingegen umfasst [Daten](#datenparallelisierung)-, [Tensor](#Tensor-Parallelisierung)- und [Sequenzparallelisierung](#datenparallelisierung), bei denen die einzelnen Elemente des Rechengraphs selbst, n√§mlich die Tensoren[^tensor] und Rechenoperationen, auf mehrere Rechenknoten verteilt werden.

Das Hauptproblem bei der automatischen Herleitung einer Parallelisierungsstrategie, die sowohl Inter- als auch Intra-Operator-Parallelisierung umfasst, ist der gewaltige Suchraum. Die Autoren von Alpa l√∂sen dieses Problem durch eine zweistufige Herangehensweise: F√ºr jede m√∂gliche Inter-Operator-Konfiguration suchen sie die (nahezu) optimale Intra-Operator-Konfiguration. Bei der Suche eines optimalen Intra-Operator-Parallelisierungsplans wird das Teilnetz des Clusters sowie die Partitionierung des Rechengraphs fest vorgegeben. Die Autoren formulieren die Berechnung des Intra-Operator-Plans als ein Integer-Linear-Programming-Problem, bei dem die Summe der Berechnungs-, Kommunikations- und Resharding-Kosten[^resharding] minimiert wird. Bei der Suche des Inter-Operator-Plans hingegen minimieren sie die maximale Latenz der gesamten Pipeline mittels Dynamic Programming.

Um die Komplexit√§t weiter zu reduzieren, wenden sie zudem einige Heuristiken an wie eine Einschr√§nkung der Submesh-Formen, die Vereinfachung des Rechengraphs, das Clustern von Operationen in Schichten mit √§hnlichem Rechenaufwand, oder das fr√ºhe Beenden der Suche bei Erreichen einer Maximallatenz.

[^resharding]: *Resharding* bezeichnet eine Neuverteilung bereits verteilter Eingabematrizen, die aufgrund aufeinander folgender Operatoren mit verschiedenen Verteilungsstrategien n√∂tig wird. Angenommen der Operator $o_{i-1}$ produziert eine Matrix, deren Reihen auf mehrere Rechenknoten verteilt sind, doch der Operator $o_i$ erwartet eine Matrix, deren Spalten auf mehrere Rechenknoten verteilt sind. Dann muss mittels einer [All-to-All](../communication-pattern/index.de.md)-Operation diese Matrix zun√§chst entsprechend der Eingabespezifikationen von $o_i$ neu verteilt werden.
[^tensor]: Eingabe- und Ausgabetensoren sowie die Parameter der Operatoren

@liuColossalAutoUnifiedAutomation2023 stellen mit ColossalAuto eine Strategie zur automatisierten Planung von Intra-Operator-Parallelisierung mit Activation Checkpointing vor. Sie bauen hierbei auf die Arbeit von Alpa auf, jedoch betrachten sie Inter-Operator-Parallelisierung nicht.

- Galvatron-BMW üöß
- √úbersicht üöß

## TODO

- Optimierungsrichtingen, z.B. kommunikation verringern ode verstecken, verteilung f√ºr bessere auslastung, lange sequenzen, ...

## Referenzen

{{bibliography}}
[^8]: 
