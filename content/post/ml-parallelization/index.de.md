---
title: Parallelisierung des Maschinellen Lernens
description: Dieser Artikel beschreibt verschiedene Ans√§tze zur Parallelisierung von Trainings-Algorithmen im Maschinellen Lernen.
date: 2024-01-28 09:04:28+0800
image: cover.jpg
toc: true
math: true
draft: true
categories:
   - distributed-computing
   - machine-learning
weight: 1
---

# Parallelisierung des Maschinellen Lernens

J√ºngste Forschungen √ºber skalierbare Architekturen wie den Transformer [@vaswaniAttentionAllYou2017] haben gezeigt, dass die Leistungsf√§higkeit von neuronalen Netzen mit der Anzahl ihrer Parameter korreliert [@kaplanScalingLawsNeural2020] und dabei mit besonders gro√üen Netzwerken beeindruckende Ergebnisse erzielt [@openaiIntroducingChatGPT2022; @openaiGPT4TechnicalReport2023]. Dies f√ºhrte j√ºngst zu einem Wettlauf um die gr√∂√üten Modelle und damit verbunden einem drastischen Anstieg der Anforderungen an die Trainings- und Inferenz-Hardware. Diese Architekturen lassen sich zwar quasi beliebig vergr√∂√üern, doch sind der Skalierung in der Praxis durch die Hardware Grenzen gesetzt [@narayananEfficientLargescaleLanguage2021]. So hat sich beispielsweise die Zahl der Parameter der gr√∂√üten Modelle von 2018 bis 2021 vertausendfacht, w√§hrend die Speicherkapazit√§t der popul√§rsten Beschleuniger, Nvidia-Grafikkarten, sich in demselben Zeitraum lediglich verdoppelt hat. Mit der Gr√∂√üe der neuronalen Netze steigt auch die Trainingszeit und die Anzahl der f√ºr das Training ben√∂tigten Daten stetig an. Um also das Training immer gr√∂√üerer Modelle zu erm√∂glichen, ist die Verteilung der Rechenleistung auf mehrere Rechenknoten unumg√§nglich.

![Ein Liniendiagramm, das die Gr√∂√üe der neuronalen Netze in den Jahren von 2018 bis 2021 darstellt. Es sind die folgenden KI-Modelle eingezeichnet: 2018: ELMo (94 Mio.), BERT-L (340 Mio.); 2019: GPT-2 (1,5 Mrd.), Megatron-LM (8,3 Mrd.); 2020: Turing-NLG (17,2 Mrd.), GPT-3 (175 Mrd.). Alle diese Modelle liegen auf einer Geraden in einem logarithmischen Ma√üstab.](img/neural-network-size.png)
: Gr√∂√üenentwicklung von neuronalen Netzen √ºber die Zeit. Im Zeitraum von 2018 bis 2021 ist die Anzahl der Parameter in modernen neuronalen Netzen exponentiell angestiegen. Quelle: [@narayananEfficientLargescaleLanguage2021].

Dieser Artikel soll eine Einf√ºhrung in das verteilte Rechnen mit dem Fokus auf das maschinellen Lernen bieten. Zun√§chst werden die Grundlagen erl√§utert, anschlie√üend werden mit Daten- und Modellparallelisierung sowie Mixture of Experts verschiedene Formen der Parallelisierung eingef√ºhrt, die in der j√ºngsten Literatur diskutiert werden und in der Praxis bereits rege Anwendung finden.

## Grundlagen

Das Training immer gr√∂√üerer neuronaler Netze stellt die Entwickler vor zwei grunds√§tzliche Probleme: 

1. die f√ºr das Training n√∂tige Rechenzeit steigt mit der Anzahl der zu trainierenden Parameter an,
2. der Speicher aktueller Grafikkarten ist zu klein f√ºr die gro√üen Modelle inklusive ihrer Aktivierungen und Gradienten.

Diese Probleme k√∂nnen jeweils durch die Verteilung des Trainings auf mehrere Rechenknoten gel√∂st werden. Das Training von Modellen der Gr√∂√üe von ChatGPT und GPT-4 wird durch Parallelisierung √ºberhaupt erst m√∂glich gemacht. Doch f√ºhrt diese Parallelisierung zu einem weiteren Problem:

3. der durch Kommunikation und Synchronisation der Rechenknoten entstehende Mehraufwand steigt mit der Anzahl der Rechenknoten an.

### Scale Out Vs. Scale Up üöß

- Scale Up nur begrenzt m√∂glich, scale out erlaubt in der Theorie beliebige Skalierungsfaktoren (siehe [unten](#skalierbarkeit)).
- Effizienzsteigerung eine Form von Scle UP?

### Skalierbarkeit

- Ahmdahl: strong scalingüöß
- Gustavson: weak scalingüöß

Soll durch eine Parallelisierung die Rechenzeit verk√ºrzt werden, ist eine Betrachtung des Skalierungsverhaltens des Programms vonn√∂ten. Nicht jedes Programm profitiert gleicherma√üen von einer Verteilung auf mehrere Prozesse. Die Beschleunigung $S_p$ (engl.: *Speedup*), die durch die Verteilung auf $p$ Prozesse erreicht wird, ist das Verh√§ltnis zwischen der Rechenzeit[^Latenz] eines einzelnen Prozesses $T_1$ zur Rechenzeit von $p$ Prozessen $T_p$:

$$
S_p = \frac{T_1}{T_p}
$$
Auf Basis der Beschleunigung kann die Effizienz $E_p$ als das Verh√§ltnis der Beschleunigung $S_p$ zur Anzahl der Prozesse $p$ berechnet werden: 

$$
E_p = \frac{S_p}{p} = \frac{T_1}{T_p \cdot p}
$$
Die Effizienz (engl.: *efficiency*) gibt an, inwiefern das betrachtete Programm durch die Verteilung auf $n$ Prozesse profitiert. Bei einer Effizienz von $E_p = 1$ skaliert das Programm *linear*, also eine Erh√∂hung der Rechenkerne um einen Faktor $f$ f√ºhrt zu einer Erh√∂hung der Beschleunigung um den gleichen Faktor. Eine Effizienz von $E_p < 1$ deutet auf ein sub-lineares Skalierungsverhalten hin und eine Effizienz von $E_p > 1$ auf eine super-lineare Skalierung. Das folgende Schaubild verdeutlicht diese Beziehung:

![Speedup](img/scalability.svg)
: Verh√§ltnis der Beschleunigung (Speedup, $S_P$) zur Anzahl der parallelen Prozesse (\#Workers, $P$): Entspricht die Beschleunigung der Erh√∂hung der parallelen Prozesse ($S_P = P$, blaue Linie), skaliert das Programm linear; ist die Beschleunigung gr√∂√üer als die Anzahl der Prozesse ($S_P > P$, gr√ºner Bereich), liegt eine super-lineare Skalierung vor und im umgekehrten Fall ($S_P < P$, roter Bereich) skaliert das Programm sub-linear.

Zwar gibt es einige Algorithmen, die eine linear oder sogar super-linear skalieren [@aklSuperlinearPerformanceRealTime2004;@ristovSuperlinearSpeedupHPC2016], doch ist in der Praxis eine sub-lineare Skalierung aufgrund durch die Parallelisierung zus√§tzlich anfallender Aufgaben wie Kommunikation und Synchronisation zwischen den Prozessen die Regel [@mccoolStructuredParallelPrograming2012].

In der Forschung zur Parallelisierung von neuronalen Netzen wird h√§ufig nicht die Rechenzeit als Basis zur Berechnung der Beschleunigung herangezogen, da sich aufgrund der i.d.R. notwendigen Gradientenakkumulierung eine super-lineare Beschleunigung nur schwer erreichen l√§sst. Beispielsweise berechnen [@rajbhandariZeROMemoryOptimizations2020] die Beschleunigung in Bezug auf die Anzahl der Flie√ükommazahloperationen pro Sekunde (FLOPs).  üöß

[^Latenz]: Der Fachbegriff f√ºr die Rechenzeit ist "Latenz" (engl.: *latency*). 

### Klassifikation von Rechnerarchitekturenüöß

Flynn's taxonomy:

- SISD
- MISD
- SPMD/SIMD/SIMT
- MIMD/MPMD

### Rechnerhardware und Infrastruktur

Matrix-Operationen bilden die Grundlage f√ºr das Rechnen mit neuronalen Netzen. Diese lassen sich i.d.R. effizient parallelisieren, wodurch GPUs aufgrund ihrer tausenden von spezialisierten Prozessoren geeigneter f√ºr das Rechnen mit neuronalen Netzen sind als CPUs. Moderne Rechencluster setzen daher haupts√§chlich auf GPUs oder speziell f√ºr die Anwendung im maschinellen Lernen entwickelte Neural Processing Units (NPUs). Beispiele f√ºr NPUs sind Googles TPU [@jouppiInDatacenterPerformanceAnalysis2017a; @googleSystemArchitectureCloud2023] und Huaweis Ascend-Plattform [@huaweiAscendComputing2024].

Da der Speicher einzelner GPUs begrenzt ist, aber die Gr√∂√üe moderner neuronaler Netze exponentiell anw√§chst [@narayananEfficientLargescaleLanguage2021], kommen in der k√ºnstlichen Intelligenz vermehrt verteilte Architekturen zum Einsatz.

Ein auf GPUs basierendes Cluster besteht aus mehreren Racks mit mittels Ethernet oder InfiniBand √ºber Netzwerkswitches verbundenen Rechnern, an welchen wiederum √ºber PCIe oder herstellerspezifische Sockets wie SXM mehrere GPUs angeschlossen sind. Die GPUs wiederum k√∂nnen mittels bandbreitenstarker Direktverbindungen und spezialisierter Switches zus√§tzlich untereinander vernetzt werden, um h√∂here Bandbreiten bei √úbertragungen zwischen den GPUs eines Rechners zu erm√∂glichen[^NVLink]. Die Verbindungen in den unteren Schichten dieses Baumes haben i.d.R. die h√∂chste Bandbreite, w√§hrend in den dar√ºber liegenden Schichten die Bandbreite weiter abnimmt.

![Topologie eines GPU-Clusters: Vier Server sind √ºber einen Switch mittels d√ºnner Linien verbunden, unterhalb der Server sind jeweils vier GPUs abgebildet, die mit dem Server √ºber dickere Linien verbunden sind. Zus√§tzlich sind die vier GPUs jedes Servers √ºber sehr dicke Linien untereinander vernetzt.](img/gpu-cluster-topology.svg)
: Die Netzwerktopologie eines GPU-Clusters. Die Dicke der Verbindungslinien zwischen den Komponenten ist proportional zu deren Bandbreite.

- SuperPOD üöß (xPod?) https://docs.nvidia.com/https://docs.nvidia.com/dgx-superpod-reference-architecture-dgx-h100.pdf

Im Gegensatz zur hierarchischen Topologie in GPU-Clustern sind TPUs in einer 2D-Matrix angeordnet. Jede TPU ist selbst netzwerkf√§hig und √ºber schnelle Direktverbindungen mit jeweils vier anderen TPUs verbunden. Insgesamt bilden die TPUs einen Torus, was u.a. effiziente All-Reduce-Operationen erm√∂glicht [@jouppiDomainspecificSupercomputerTraining2020].

![Topologie eines TPU-Clusters: Die TPUs sind in einer 2D-Matrix angeordnet und sind horizontal und vertikal miteinander verbunden. Die TPUs an den R√§ndern der Matrix sind mit den jeweiligen TPUs an den anderen Enden verbunden, sodass die Struktur insgesamt einen Torus bildet.](img/tpu-cluster-topology.svg)
: Die Netzwerktopologie eines TPU-Clusters. Die Farben dienen lediglich der besseren Unterscheidung der Verbindungen.

[^NVLink]: NVIDIA bietet diesbez√ºglich NVLink und NVSwitch an [@nvidiaNVLinkNVSwitchFastest2023].

## Ans√§tze

Inspiriert durch die Arbeit von @zhengAlpaAutomatingInter2022 verwende ich in diesem Artikel eine ganzheitliche Sicht auf die Parallelisierung im maschinellen Lernen. Jedes ML-Modell kann in Form eines Graphs dargestellt werden, dessen Knoten entweder Daten (Eingabedaten, Parameter oder Aktivierungen) oder Operationen (Addition, Multiplikation, ...) darstellen. Bei der Verteilung eines Modells werden Teile des Graphs entlang einer oder mehrerer Dimensionen partitioniert und auf verschiedene Ger√§te verteilt. Die Operationen werden schlie√ülich auf allen Ger√§ten ausgef√ºhrt, auf denen ein Teil der Eingabedaten liegt. In der Literatur wird in Abh√§ngigkeit von den partitionierten Daten in Datenparallelisierung, Tensor-Parallelisierung und Pipeline-Parallelisierung unterschieden:

![Es sind vier Graphen zu sehen.](img/parallelism-overview.svg)

: √úbersicht √ºber verschiedene g√§ngige Formen der Parallelisierung: Ein Rechengraph (a) kann auf verschiedene Arten verteilt werden. Bei der Datenparallelisierung (b) werden die Daten auf mehrere Knoten verteilt, bei der Tensor-Parallelisierung (c) die Modellparameter und bei der Pipeline-Parallelisierung (d) der Graph selbst. Gegebenenfalls m√ºssen zus√§tzlich Kommunikationsoperationen durchgef√ºhrt werden.

Bei der Datenparallelisierung werden die Eingabedaten sowie Aktivierungen auf mehrere Ger√§te verteilt, bei der Tensor-Parallelisierung hingegen die Modellparameter und Aktivierungen, w√§hrend bei der Pipeline-Parallelisierung ganze Teile des Graphs verteilt werden. Diese Formen der Parallelisierung werden im Folgenden n√§her erl√§utert. Obwohl sich diese Formen der Parallelisierung gleicherma√üen auf die Inference anwenden lassen, liegt der Fokus in den folgenden Ausf√ºhrungen auf das Training, da der Inference-Prozess der Vorw√§rtsberechnung beim Training entspricht.

- welche Ans√§tze erm√∂glichen strong/weak scaling?üöß

## Datenparallelisierung

Die Datenparallelislierung ist die einfachste und am weitesten verbreitete Form der Parallelisierung im maschinellen Lernen. Beim der Datenparallelisierung werden die Eingabedaten auf mehrere Rechenknoten verteilt, was das Training auf gr√∂√üeren Datens√§tzen erm√∂glicht[^spmd]. Jeder Rechenknoten beh√§lt dabei das vollst√§ndige Modell im Speicher, wodurch die Gr√∂√üe des Modells weiterhin durch den verf√ºgbaren Speicher begrenzt wird [@rajbhandariZeROMemoryOptimizations2020]. Jeder Knoten berechnet unabh√§ngig voneinander die Ausgabe und Gradienten und synchronisiert anschlie√üend die lokalen Gradienten mit den √ºbrigen Knoten mittels einer [All-Reduce](/blog/de/p/kommunikationsmuster/#all-reduce)-Operation. Abschlie√üend aktualisiert jeder Knoten seine lokalen Modellparameter auf Basis der akkumulierten Gradienten. In der nachfolgenden Graphik werden die Eingabedaten entlang der Batch-Dimension auf die verschiedenen Rechenknoten verteilt. Jeder Rechenknoten berechnet anschlie√üend unabh√§ngig voneinander die Ausgabe.

[^spmd]: Datenparallelisierung kann auch als "[Single Program Multiple Data](https://de.wikipedia.org/wiki/Single-Program_Multiple-Data)" (SPMD) betrachtet werden, wobei im maschinellen Lernen das KI-Modell mit seinen Instruktionen und Modellparametern das ausgef√ºhrte Programm darstellt.

![SVG image](img/data-parallel.svg)
: Datenparalleles Training von neuronalen Netzen. Das Modell wird auf $n$ Knoten repliziert, w√§hrend die Trainingsdaten gleichm√§√üig auf die Rechenknoten verteilt werden. Hier werden die Daten entlang der Batch-Dimension verteilt.

Alternativ zur All-Reduce-Operation k√∂nnen die Modellparameter durch einen Parameter-Server aktualisiert werden [@liScalingDistributedMachine2014]. Die Aufgabe des Parameter-Servers ist die zentrale Aktualisierung der Modell-Parameter auf Basis der Gradienten aller teilnehmenden Prozesse. Da allerdings der Server mit allen Prozessen kommunizieren muss, skaliert er nur begrenzt. In der Praxis wird dieser Ansatz aktuell selten gew√§hlt.

Da beim datenparallelen Training alle Rechenknoten alle Modellparameter mitsamt der zugeh√∂rigen Gradienten und Optimierer-Zust√§nden speichern m√ºssen, ist die Modellgr√∂√üe durch den verf√ºgbaren Speicher der einzelnen Knoten begrenzt. @rajbhandariZeROMemoryOptimizations2020 stellen deshalb den *Zero Redundancy Optimizer* (ZeRO) vor, der durch die gleichm√§√üige Verteilung der Optimierer-Zust√§nde, Gradienten und Modellparameter auf alle Rechenknoten den Speicherverbrauch drastisch reduziert. Werden nur die Optimierer-Zust√§nde und Gradienten verteilt, entspricht das Kommunikationsvolumen von ZeRO dem des datenparallelen Trainings. Bei zus√§tzlicher Partitionierung der Modellparameter erh√∂ht es sich um 50%[^zeropp]. Die Autoren weisen in Experimenten nach, dass das datenparallele Training mit diesem Optimierer in Bezug auf FLOPs super-linear skaliert.

[^zeropp]: In der Praxis kann diese Erh√∂hung des Kommunikationsvolumens einen nicht-vernachl√§ssigbaren Einfluss auf den Trainingsdurchsatz haben. Daher kombinieren @wangZeROExtremelyEfficient2023 Quantisierung mit einer neuen Platzierungsstrategie, um den Overhead durch die Kommunikation zu reduzieren.

PyTorch unterst√ºtzt mit PyTorch DDP [@liPyTorchDistributedExperiences2020a] schon seit l√§ngerem datenparalleles Training nativ. Die Unterst√ºtzung f√ºr die zus√§tzliche Partitionierung von Optimierer-Zust√§nden, Gradienten und Modellparametern wurde mit PyTorch FSDP [@zhaoPyTorchFSDPExperiences2023] in PyTorch 2.0 eingef√ºhrt.

- Einfluss Datenparallelisierung auf den Durchsatz üöß

## Modellparallelisierung

Bei der Modellparallelisierung werden die Parameter des Modells auf verschiedene Rechenknoten verteilt. Es wird hierbei zwischen Tensor- und Pipeline-Parallelisierung unterschieden.

### Pipeline-Parallelisierung

Bei der Pipeline-Parallelisierung [@huangGPipeEfficientTraining2019; @narayananPipeDreamGeneralizedPipeline2019; @fanDAPPLEPipelinedData2020] wird das Modell in Abschnitte aufeinander folgender Operatoren zerteilt und diese Abschnitte verschiedenen Rechenknoten zugeordnet. Die Aktivierungen der einzelnen Abschnitte werden mittels Punkt-zu-Punkt-Kommunikation[^sendrecv] auf den Rechenknoten mit dem jeweils folgenden Abschnitt √ºbertragen.

Bei der Aufteilung des Modells wird √ºblicherweise darauf geachtet, dass das Kommunikationsvolumen an den Schnittstellen zwischen zwei Abschnitten m√∂glichst gering ist. Zudem sollte darauf geachtet werden, dass jeder Abschnitt eine √§hnliche Berechnungszeit hat, um eine gleichm√§√üige Auslastung der Knoten zu gew√§hrleisten. Gleichm√§√üige Modelle, wie der Transformer, k√∂nnen einfach an den Schichtengrenzen zerteilt und gleichm√§√üig auf alle Knoten verteilt werden [@narayananEfficientLargescaleLanguage2021], da an den Schichtengrenzen das Kommunikationsvolumen √ºblicherweise gering ist.

[^sendrecv]: Die zugeh√∂rigen Kommunikationsprimitive werden *Send* (senden) und *Recv* (receive, empfangen) genannt.

![](img/pp-naive.svg)

: Beispiel einer Pipeline-Parallelisierung: Das Modell mit acht Schichten ist gleichm√§√üig auf vier Beschleuniger verteilt. Der erste Beschleuniger berechnet die Aktivierungen der ersten beiden Schichten und leitet das Ergebnis an den zweiten Beschleuniger weiter, welcher wiederum die Aktivierungen f√ºr die n√§chsten beiden Schichten berechnet. Der Vorgang wiederholt sich solange, bis auch der vierte Beschleuniger die Aktivierungen berechnet hat. Anschlie√üend berechnet dieser den Fehler und startet die R√ºckw√§rtsberechnung f√ºr die letzten beiden Schichten. Die Gradienten der Aktivierungen werden anschlie√üend an den dritten Beschleuniger weitergeleitet, welcher wiederum die R√ºckw√§rtsberechnung f√ºr die vorletzten beiden Schichten durchf√ºhrt. Dieser Vorgang wiederholt sich solange, bis die Gradienten f√ºr alle Schichten des Modells zur Verf√ºgung stehen. Abschlie√üend werden die Parameter auf allen vier Beschleunigern gleichzeitig aktualisiert. Danach beginnt der gesamte Prozess von neuem mit dem n√§chsten Mini-Batch. W√§hrend ein Beschleuniger rechnet sind alle anderen idle.

Diese Form der Parallelisierung erm√∂glicht prinzipiell ein beliebiges Skalieren der Modelltiefe. Der Vorteil dabei ist das bei geeigneter Wahl der Abschnitte geringe Kommunikationsvolumen und ist damit auch f√ºr Netzwerke mit geringerer Bandbreite geeignet. Allerdings f√ºhrt die Hintereinanderausf√ºhrung der Abschnitte zu Phasen geringer Rechenlast, die sog. *bubble time*, bei denen nur ein Teil der reservierten Rechenknoten genutzt wird. Ein Hauptziel bei der Optimierung von pipeline-parallelen Setups ist daher die Minimierung dieser Bubble-Zeit und damit die Maximierung der Auslastung des Clusters.

Durch die Verwendung von **Micro-Batches** [@huangGPipeEfficientTraining2019][^gpipe] kann die Bubble-Zeit verringert werden. Dabei werden die herk√∂mmlichen Mini-Batches in kleinere Micro-Batches zerteilt[^micro] und nacheinander in das Modell eingegeben. Die Parameter werden erst aktualisiert, nachdem die Gradienten aller Micro-Batches vorliegen. Die geringere Berechnungszeit von kleineren Batches und die √ºberlappende Bearbeitung der Micro-Batches durch verschiedene Rechenknoten f√ºhrt zu einer besseren Auslastung der Rechenknoten. Je h√∂her die Anzahl der Micro-Batches, desto geringer ist der Anteil der Bubble-Zeit [@narayananEfficientLargescaleLanguage2021].

[^gpipe]: GPipe
[^micro]: Diese Technik wird mit *Gradientenakkumulation* bezeichnet, da die Gradienten mehrerer (Micro-)Batches aufsummiert werden, bevor die Parameter aktualisiert werden.

![](img/pp-microbatches.svg)

: Beispiel von Pipeline-Parallelisierung mit Micro-Batches: Der Mini-Batch wurde in acht Micro-Batches aufgeteilt, welche nacheinander in das Modell eingegeben werden. W√§hrend beispielsweise der zweite Beschleuniger die Vorw√§rtsberechnung f√ºr den ersten Micro-Batch durchf√ºhrt, beginnt der erste Beschleuniger bereits mit der Berechnung des zweiten Micro-Batches. Das gleiche gilt f√ºr die R√ºckw√§rtsberechnung. Erst nachdem die R√ºckw√§rtsberechnung f√ºr alle Micro-Batches beendet wurde, werden die Parameter auf allen Beschleunigern gleichzeitig aktualisiert.

Allerdings erh√∂ht sich dabei der f√ºr die Aktivierungen n√∂tige Speicheraufwand[^activations], denn f√ºr jeden Micro-Batch m√ºssen jeweils die Aktivierungen solange gespeichert werden, bis die R√ºckw√§rtsberechnung aller nachfolgenden Micro-Batches abgeschlossen ist. Das f√ºhrt bei $N$ Micro-Batches zu einem maximalen Speicherverbrauch f√ºr Aktivierungen von $N \cdot M_a$, wobei $M_a$ den Speicherverbrauch pro Micro-Batch bezeichnet. Der **1F1B-Schedule**[^1f1b] [@narayananPipeDreamGeneralizedPipeline2019; @narayananEfficientLargescaleLanguage2021; @fanDAPPLEPipelinedData2021] l√∂st dieses Problem durch die unverz√ºgliche R√ºckw√§rtsberechnung eines Micro-Batches, sobald die Vorw√§rtsberechnung desselben f√ºr alle $S$ Pipeline-Abschnitte abgeschlossen ist. Der 1F1B-Schedule operiert in zwei Phasen: In der Aufw√§rmphase (engl.: *warmup*) nimmt jeder Knoten zun√§chst nur Vorw√§rtsberechnungen vor. Es kommt hierbei allerdings auch zu Bubble-Zeiten. Wenn die R√ºckw√§rtsberechnung des ersten Micro-Batches beginnt, wechselt 1F1B in die Steady-State-Phase, in welcher jeder Knoten jeweils eine Vorw√§rts- und R√ºckw√§rtsberechnung abwechselnd ausf√ºhrt. Nachdem die Vorw√§rtsberechnungen f√ºr alle Micro-Batches abgeschlossen wurden, wechselt 1F1B in die Abk√ºhlungsphase (engl.: *cooldown*), in der nur noch R√ºckw√§rtsberechnungen vorgenommen werden und es wieder zu Bubble-Zeiten kommt.

Bei $D$ Beschleunigern reduziert sich der f√ºr die Speicherung der Aktivierungen n√∂tige maximale Speicheraufwand auf $D \cdot M_a$, da zu jedem Zeitpunkt maximal $D$ Micro-Batches offen sind. Bei synchronen 1F1B-Schedules wie in DAPPLE [@fanDAPPLEPipelinedData2021] und PipeDream-Flush [@narayananEfficientLargescaleLanguage2021] werden die Modell-Parameter aller Knoten gleichzeitig aktualisiert, sobald die R√ºckw√§rtsberechnung des Mini-Batches abgeschlossen ist (der sog. *Pipeline-Flush*). Die Bubble-Zeit von 1F1B mit Pipeline-Flush entspricht der von GPipe.

[^activations]: Die Anzahl der n√∂tigen Aktivierungen kann zwar mittels *Activation Checkpointing* reduziert werden, doch m√ºssen die Eingabe-Aktivierungen jeder Modell-Partition weiterhin gespeichert werden. Daher bleibt der Speicheraufwand proportional zur Anzahl der Micro-Batches.
[^1f1b]: 1F1B steht f√ºr "*one forward one backward*"

![](img/pp-1f1b.svg)

: Beispiel eines 1F1B-Schedules mit Pipeline-Flush nach DAPPLE [@fanDAPPLEPipelinedData2021] und PipeDream-Flush [@narayananEfficientLargescaleLanguage2021]: Die ersten drei Beschleuniger berechnen jeweils die Aktivierungen f√ºr die ersten vier, drei bzw. zwei Micro-Batches und warten daraufhin auf die Gradienten des ersten Micro-Batches. Der letzte Beschleuniger berechnet lediglich die Aktivierungen f√ºr den ersten Micro-Batch und berechnet unverz√ºglich dessen Gradienten. Die Gradienten werden daraufhin zur√ºckgesendet, woraufhin der vierte Beschleuniger mit der Vorw√§rtsberechnung des n√§chsten Micro-Batches beginnt. Dieser ist nun in der Steady-State-Phase, in der er Vorw√§rts- und R√ºckw√§rtsberechnungen solange abwechselnd ausf√ºhrt, bis alle Vorw√§rtsberechnungen abgeschlossen wurden. Gleiches gilt f√ºr die anderen Beschleuniger.

Durch eine asynchrone Parameteraktualisierung minimiert **PipeDream** [@narayananPipeDreamGeneralizedPipeline2019] die Bubble-Zeit beim √úbergang zwischen den Mini-Batches. Dabei beginnt die Vorw√§rtsberechnung des n√§chsten Mini-Batches, w√§hrend die R√ºckw√§rtsberechnung des vorherigen noch nicht abgeschlossen ist; es werden also die Aufw√§rm- und Abk√ºhlungsphase der beiden Mini-Batches miteinander verschmolzen. Zwar wird dadurch die maximale Auslastung beim √úbergang zwischen den Mini-Batches erreicht, doch muss f√ºr jeden aktiven Micro-Batch jeweils eine Version der Modellparameter vorgehalten werden. **PipeDream-2BW** [@narayananEfficientLargescaleLanguage2021] reduziert die Anzahl der Modellparameter-Versionen auf zwei, indem die Aktualisierung der Parameter erst nach Abschluss des Mini-Batches erfolgt. Stochastic Gradient Descend (SGD) mit asynchroner Parameteraktualisierung ist allerdings nicht mathematisch √§quivalent zum herk√∂mmlichen SGD und dessen Konvergenz daher theoretisch nicht garantiert. Nichtsdestotrotz k√∂nnen @narayananEfficientLargescaleLanguage2021 experimentell nachweisen, dass sich die asynchrone Parameteraktualisierung nicht nennenswert auf das Konvergenzverhalten auswirkt.

![Async 1F1B](img/pp-1f1b-async.svg)

: Beispiel eines asynchronen 1F1B-Schedules mit doppelt gepufferten Parametern nach @narayananEfficientLargescaleLanguage2021: In der Abk√ºhlungsphase des aktuellen Mini-Batches beginnt bereits die Aufw√§rmphase des n√§chsten Mini-Batches. Nachdem die R√ºckw√§rtsberechnung des letzten Micro-Batches des aktuellen Mini-Batches abgeschlossen ist, werden auf einem Rechenkoten die Parameter aktualisiert. Es m√ºssen die Parameter des letzten Mini-Batches solange vorgehalten werden, bis die R√ºckw√§rtsberechnung des $D - 1$ten Micro-Batches abgeschlossen wurde, denn die Vorw√§rtsberechnung dieser Micro-Batches wurde mit den alten Parametern vorgenommen.

Der **verschachtelte** (*interleaved*) **1F1B-Schedule** [@narayananEfficientLargescaleLanguage2021] ist eine Optimierung des 1F1B-Schedules mit reduzierter Bubble-Zeit. Diese Reduktion wird dadurch erreicht, dass ein Rechenknoten f√ºr mehrere nicht aufeinander folgende Partitionen des Modells zust√§ndig ist (*model chunks*). Da die Partitionen des Modells nun kleiner sind, k√∂nnen sie schneller berechnet werden. Damit k√∂nnen die Micro-Batches schneller in die Pipelines eingegeben werden. Bei $V$ Modellpartitionen pro Knoten verringert sich die Bubble-Zeit um den Faktor $V$, d.h. die relative Bubble-Zeit sinkt proportional zur Anzahl der Modellpartitionen pro Knoten. Im Gegenzug erh√∂ht sich der P2P-Kommunikationsbedarf ebenfalls um den Faktor $V$. Da sich der relative Anteil der Bubble-Zeit an der Trainingsdauer bei der Erh√∂hung der Microbatches pro Batch verringert, kann der erh√∂hte Kommunikationsbedarf den Trainingsdurchsatz ab einer bestimmten Anzahl an Microbatches auch verringern. Daher ist der verschachtelte 1F1B-Schedule vor allem bei kleineren Batches von Vorteil. Der Speicheraufwand des verschachtelten Schedules entspricht dem von 1F1B.

![](img/pp-1f1b-interleaved.svg)

: Beispiel des verschachtelten 1F1B-Schedules: Das Modell mit acht Schichten wurde in acht gleichf√∂rmige Abschnitte zerteilt und diese so auf vier Beschleuniger verteilt, dass Beschleuniger 1 f√ºr die erste und f√ºnfte Schicht zust√§ndig ist, Beschleuniger 2 f√ºr die zweite und sechste, usw. Daher durchl√§uft jeder Micro-Batch jeden Beschleuniger zwei Mal.

@liChimeraEfficientlyTraining2021 erreichen mit **Chimera** eine bessere Auslastung durch die Verwendung mehrerer paralleler Pipelines. Diese Pipelines verwenden die verf√ºgbaren Rechenknoten in jeweils entgegengesetzter Richtung. Dies f√ºhrt zwar zu einer Verringerung der Bubble-Zeit, allerdings ist pro Pipeline jeweils eine Kopie der Modellparameter n√∂tig. Chimera kann als Pipeline-Parallelisierung mit Datenparallelisierung auf Ebene der Micro-Batches betrachtet werden. Daher ist neben der P2P-Kommunikation zus√§tzlich ein All-Reduce zwischen Knoten mit den gleichen Modellschichten n√∂tig, um nach Abschluss der R√ºckw√§rtsberechnung die Gradienten zu synchronisieren. Diese zus√§tzlichen All-Reduce-Operationen k√∂nnen allerdings w√§hrend der Bubble-Zeit am Ende der Mini-Batch-Berechnung durchgef√ºhrt werden.

![Chimera](img/pp-chimera.svg)

: Beispiel von Chimera mit zwei Pipelines auf vier Beschleunigern: Die Pipeline "down" beginnt beim Beschleuniger 1 und endet beim Beschleuniger 4. Die Pipeline "up" l√§uft in entgegengesetzter Richtung. Jede Pipeline besitzt eine eigene Kopie der Modell-Parameter. Micro-Batches 1, 2, 5, und 6 werden durch die Pipeline "down" berechnet, Micro-Batches 3, 4, 7 und 8 hingegen durch "up". Jeder Rechenknoten arbeitet jeweils abwechselnd an den beiden Pipelines, wobei in einem Slot zwei Vorw√§rtsberechnungen oder eine R√ºckw√§rtsberechnung stattfinden kann. Die Parameter werden auf allen Rechenknoten synchronisiert berechnet.

Bei der R√ºckw√§rtsberechnung werden die Ableitungen des Fehlers in Bezug auf die Aktivierungen und Parameter berechnet, doch w√§hrend erstere zur R√ºckw√§rtsberechnung in vorherigen Schichten ben√∂tigt werden, werden letztere erst bei der Parameteraktualisierung ben√∂tigt (siehe nachfolgende Grafik). Diesen Umstand machen sich @qiZeroBubblePipeline2023 zunutze und zerlegen die R√ºckw√§rtsberechnung f√ºr einen Micro-Batch in zwei Schritte: der R√ºckw√§rtsberechnung f√ºr die Aktivierungen, welche m√∂glichst fr√ºhzeitig vollzogen werden sollte, und die R√ºckw√§rtsberechnung f√ºr die Parameter, welche zu einem beliebigen Zeitpunkt zwischen der R√ºckw√§rtsberechnung f√ºr die Aktivierungen und der Parameteraktualisierung stattfinden kann. Damit kann die R√ºckw√§rtsberechnung der vorherigen Schicht bereits beginnen, bevor die komplette R√ºckw√§rtsberechnung abgeschlossen ist.

![](img/backward-graph.svg)

: Visualisierung der Vorw√§rts- und R√ºckw√§rtsberechnung. Bei der R√ºckw√§rtsberechnung werden jeweils die Ableitungen des Fehlers $E$ in Bezug auf die Parameter $\theta$ und die Aktivierungen $x$ berechnet. Dabei f√§llt auf, dass zur R√ºckw√§rtsberechnung der vorherigen Schichten nur die Ableitungen in Bezug auf die Aktivierungen ben√∂tigt werden. Die Ableitungen bez√ºglich der Parameter (Gradienten) werden erst bei der Parameteraktualisierung ben√∂tigt.

Dadurch schaffen es die Autoren einen Pipeline-Schedule[^zero-pp-sched] mit synchronen Parameterupdates zu kreieren, der in der Theorie keine Bubble-Zeit beim √úbergang zwischen den Mini-Batches aufweist. Daher bezeichnen sie ihren Ansatz als **Zero-Bubble-Pipeline-Parallelism**. Durch die zus√§tzliche Kombination mit einem verschachtelten Schedule, dessen hinterer Teil entgegengesetzt zum vorderen verl√§uft, stellen sie zudem sicher, dass die nur w√§hrend der R√ºckw√§rtsberechnung ben√∂tigten Ableitungen bzgl. der Aktivierungen schneller wieder entfernt werden k√∂nnen. In ihren Experimenten zeigen die Autoren, dass ihr Schedule bei √§hnlichem Speicherbedarf einen bis zu 23% besseren Durchsatz als 1F1B erreicht.

[^zero-pp-sched]: Die Autoren stellen mehrere Pipeline-Schedules vor, die jeweils auf die Idee der Aufspaltung der R√ºckw√§rtsberechnung basieren. Hier stelle ich lediglich den speichereffizienten Schedule vor, da die anderen gegen√ºber dem speichereffizienten Schedule kaum Vorteile aufweisen.

![](img/pp-zero-bubble.svg)

Pipeline-Parallelisierung kann auch auf Sequenzebene eingesetzt werden, was sich vor allem zur Anwendung von Transformer-Modellen auf langen Sequenzen eignet [@liangSurveyAutoParallelismLargeScale2023]. @liTeraPipeTokenLevelPipeline2021 stellen mit **TeraPipe** ein System vor, welches die Eingabedaten entlang der Sequenzdimension in Teilsequenzen ungleicher L√§nge aufteilt und nacheinander in das Modell eingibt. Bei der Aufteilung der Sequenzen wird darauf geachtet, dass jede Teilsequenz eine √§hnliche Berechnungszeit ben√∂tigt. Wie auch die oben vorgestellte Pipeline-Parallelisierung auf Operator-Ebene ist der Ansatz von TeraPipe kompatibel mit Tensor-Parallelisierung.

Auch wenn die durch Punkt-zu-Punkt-Kommunikation zwischen zwei Pipeline-Abschnitten auftretende Latenz im Vergleich zu anderen [Kommunikationsprimitiven](../kommunikationsmuster) relativ kurz ist, kann sie durch √úberlappen mit der Berechnung "versteckt" werden. @jiangMegaScaleScalingLarge2024 erreichen in ihren Experimenten eine 3%-ige Verbesserung der Auslastung, indem sie f√ºr die Send- und Recv-Kommunikation sowie die Berechnung jeweils einen eigenen CUDA-Stream verwenden.

Die folgende Tabelle gibt eine √úbersicht √ºber die Performance und den Speicherverbrauch verschiedener pipeline-paralleler Schedules nach @liChimeraEfficientlyTraining2021. $D$ ist der Grad der Pipeline-Parallelit√§t (d.h. die Anzahl der Modellabschnitte), $V$ die Anzahl an Abschnitten pro Rechenknoten, $N$ die Anzahl der Micro-Batches und $M_\theta$ und $M_a$ der f√ºr die Parameter und Aktivierungen ben√∂tigte Speicher.

| Schedule         | Bubble-Ratio                                     | Parameter                                   | Aktivierungen                                                    | Konvergenz |
|------------------|:------------------------------------------------:|:-------------------------------------------:|:----------------------------------------------------------------:|------------|
| GPipe            | $\frac{D - 1}{N + D - 1}$                        | $M_\theta$                                  | $N \cdot M_a$                                                    | Sync       |
| DAPPLE           | $\frac{D ‚àí 1}{N + D ‚àí 1}$                        | $M_\theta$                                  | $\left[ M_a, D \cdot M_a \right]$                                | Sync       |
| PipeDream-Flush  | $\frac{D - 1}{N + D - 1}$                        | $M_\theta$                                  | $\left[M_a, D \cdot M_a \right]$                                 | Sync       |
| PipeDream        | $\approx 0$                                      | $\left[ M_\theta, D \cdot M_\theta \right]$ | $\left[ M_a, D \cdot M_a \right]$                                | Async      |
| PipeDream-2BW    | $\approx 0$                                      | $2 \cdot M_\theta$                          | $\left[ M_a, D \cdot M_a \right]$                                | Async      |
| Interleaved 1F1B | $\frac{D - 1}{V \cdot \left( N + D - 1 \right)}$ | $M_\theta$                                  | $\left[ M_a, D \cdot M_a \right]$                                | Sync       |
| Chimera          | $\frac{D - 2}{2 \cdot N + D - 2}$                | $2 \cdot M_\theta$                          | $\left[ \left( \frac{D}{2} + 1 \right) M_a, D \cdot M_a \right]$ | Sync       |
| Zero Bubble      | $\approx 0$                                      | $M_\theta$                                  | $\left[ M_a, D \cdot M_a \right]$                                | Sync       |

### Tensor-Parallelisierung

Bei der Tensor-Parallelisierung werden die Parameter-Tensoren des Modells partitioniert und auf die verschiedenen Rechenknoten verteilt. Dabei wird √ºblicherweise darauf geachtet, dass die Partitionierung bei der R√ºckw√§rtsberechnung der Partitionierung bei der Vorw√§rtsberechnung entspricht, um eine Neuverteilung der Aktivierungen zu vermeiden. Die m√∂gliche Partitionierung der Operatoren und die n√∂tigen Synchronisationsoperatoren h√§ngen jedoch stark vom partitionierten Operator und dessen Verteilung ab.

Im Folgenden wird beispielhaft die verteilte Berechnung einer Matrix-Multiplikation beschrieben. Eine Matrix kann entlang einer oder mehrerer Dimensionen verteilt werden. Bei der eindimensional verteilten Berechnung einer 2D-Matrix kommt eine spalten- und reihenweise Verteilung in Frage, wobei Bl√∂cke von Spalten bzw. Reihen der Matrix auf die verschiedenen Rechenknoten verteilt werden. Bei der Multiplikation einer spaltenweise verteilten Matrix mit einer nicht verteilten Eingabe-Matrix[^input-matrix] finden sich auf den Rechenknoten die Spalten der Ergebnismatrix. Wird eine reihenweise verteilte Matrix mit einer nicht verteilten Eingabe-Matrix multipliziert, liegen auf jedem Knoten lediglich Teilsummen jeden Eintrags der Ergebnismatrix vor. Um das vollst√§ndige Endergebnis zu erhalten, ist eine jeweils AllReduce-Operation vonn√∂ten.

[^input-matrix]: Bei der Tensor-Parallelisierung betrachten wir nur die Verteilung der Parameter-Matrix. Eine Verteilung der Eingabematrix $x$ wird hingegen bei der [Datenparallelisierung](#datenparallelisierung) vorgenommen.
![Visualisierung der verteilten Matrix-Multiplikation](img/tp-matmul.svg)
: Beispiel der verteilten Multiplikation einer 2D-Matrix.

@shoeybiMegatronLMTrainingMultiBillion2020 beschreiben eine effiziente Partitionierung der Parameter eines Transformer-Blocks. Dabei werden die Query-, Key-, und Value-Matrizen des Aufmerksamkeitsblocks jeweils spaltenweise und die der darauf folgenden linearen Operation reihenweise partitioniert. Gleicherma√üen wird die erste Schicht des auf den Aufmerksamkeitsblock folgenden Multi-Layer-Perzeptron (MLP) des Transformer-Blocks spaltenweise und die zweite Schicht reihenweise verteilt. Diese Kombination erm√∂glicht einen geringen Speicherverbrauch bei minimalen Kommunikationskosten. Bei der Vorw√§rtsberechnung ist lediglich eine AllReduce-Operation nach der jeweils zweiten Schicht vonn√∂ten, w√§hrend bei der R√ºckw√§rtsberechnung eine AllReduce-Operation vor der ersten Schicht eingef√ºgt werden muss. Diese Form der Tensorparallelisierung erfordert allerdings wegen dieser AllReduce-Operationen eine hohe Bandbreite zwischen den Rechenknoten. Daher ist sie √ºblicherweise am geeignetsten f√ºr die Anwendung zwischen den GPUs eines einzelnen Rechners mit bandbreitenstarken Direktverbindungen.

![Tensor-Parallelismus in der Transformer-Architektur](img/tp-transformer.svg)
: Anwendung von Tensor-Parallelismus in einem Transformer nach @shoeybiMegatronLMTrainingMultiBillion2020. Die Operationen $f$ und $g$ sind bedingte AllReduce-Operationen. $f$ f√ºhrt einen AllReduce nur bei der R√ºckw√§rtsberechnung durch, w√§hrend $g$ einen AllReduce nur bei der Vorw√§rtsberechnung durchf√ºhrt.

Die Parameter-Matrix kann auch entlang mehrerer Dimensionen verteilt werden (2D, 2,5D, 3D) [@xuEfficient2DMethod2021; @bianMaximizingParallelismDistributed2021; @liColossalAIUnifiedDeep2022]. Obwohl sich bei der mehrdimensionalen Partitionierung in der Summe das Kommunikationsvolumen erh√∂ht, skaliert sie besser als die eindimensionale Partitionierung, da bei der eindimensionalen Partitionierung ein All-Reduce √ºber alle Knoten vonn√∂ten ist, w√§hrend sich bei der h√∂her-dimensionalen Partitionierung durch mehrere voneinander unabh√§ngige, also parallelisierbarer All-Reduce-Operationen eine geringere Latenz erreichen l√§sst üöß.

Grafik: nd-Partitionierung üöß

- make full use of bandwidth, communication brought by segmentation is basically efficient collective commüöß
- SUMMA @vandegeijnSUMMAScalableUniversal1997, 3D matmul @agarwalThreedimensionalApproachParallel1995

## Sequenz-Parallelisierung

Bei der Sequenz-Parallelisierung werden die Eingabedaten und Aktivierungen nicht entlang der Batch-, sondern entlang der Sequenz-Dimension verteilt. Es wurden in der Literatur verschiedene Varianten der Sequenz-Parallelisierung vorgestellt, die sich jeweils grunds√§tzlich voneinander unterscheiden.

@liSequenceParallelismLong2022 f√ºhren eine Form der Sequenzparallelisierung ein, bei der die Aktivierungen des Aufmerksamkeitsmoduls des Transformers anhand der L√§ngendimension partitioniert werden. Hierdurch wird der quadratische Speicherverbrauch auf $D$ Rechenknoten gleichm√§√üig verteilt, sodass auch die Verarbeitung l√§ngerer Sequenzen erm√∂glicht wird. Bei der Berechnung der Aufmerksamkeit wird jedoch jedes Token in der Sequenz mit jedem anderen Token in Relation gesetzt, sodass die Aktivierungen zwischen den Knoten ausgetauscht werden m√ºssen. Die Autoren schlagen dazu *Ring Self Attention* vor, bei der die partitionierten Key- und Value-Embeddings zwischen allen Rechenknoten zirkulieren. Bei der Verteilung auf $D$ Rechenknoten kann mit diesem Ansatz zwar der Speicheraufwand auf $\mathcal{O}(\left(\frac{S}{D}\right)^2)$ reduziert werden, doch verlangsamt sich gleichzeitig die Berechnung um den Faktor $D$[^speed-seq1]. Zudem erfordert *Ring Self Attention* eine tiefgreifende Modifikation des Aufmerksamkeitsmechanismus, weshalb diese Methode f√ºr andere Formen des Aufmerksamkeitsalgorithmus, wie z.B. Sparse Attention [@childGeneratingLongSequences2019; @qiuBlockwiseSelfAttentionLong2020; @daoFlashAttentionFastMemoryEfficient2022] oder FlashAttention [@daoFlashAttentionFastMemoryEfficient2022; @tridaoFlashAttention2FasterAttention], entsprechende Anpassungen erfordert.

[^speed-seq1]: *Ring Self Attention* verlangsamt zwar die Berechnung der Aufmerksamkeit um den Faktor $D$, doch kann die Verringerung des Speicheraufwandes eine Erh√∂hung der Batch-Gr√∂√üe erlauben, was sich wiederum positiv auf den Trainingsdurchsatz auswirken kann.

Auf Basis von FlashAttention [@daoFlashAttentionFastMemoryEfficient2022] stellen @liLightSeqSequenceLevel2023: √§hnlich zu FlashAttention, Austausch von Q, K, V zwischen den Knoten üöß

Im Gegensatz zu @liSequenceParallelismLong2022 und @liLightSeqSequenceLevel2023 parallelisieren @korthikantiReducingActivationRecomputation2022 und @jacobsDeepSpeedUlyssesSystem2023 nicht die Aktivierungen des Aufmerksamkeitsmoduls entlang der Sequenz-Dimension, sondern beschreiben wie sequenzparallele Aktivierungen an den Schnittstellen des Aufmerksamkeitsmoduls umgeformt werden m√ºssen. Hierbei bleibt allerdings der Aufmerksamkeitsmechanismus unver√§ndert und damit auch die Gr√∂√üe der Aktivierungen innerhalb des Moduls. Daher wird diese Form der Sequenzparallelisierung oft mit FlashAttention oder Sparse Attention kombiniert.

@korthikantiReducingActivationRecomputation2022 erg√§nzen die tensor-parallele Berechnung des Aufmerksamkeitsmoduls [@shoeybiMegatronLMTrainingMultiBillion2020] um eine Parallelisierung der LayerNorm- und Dropout-Operatoren. Dazu werden die Aktivierungen derer entlang der Sequenz-Dimension verteilt, sodass der f√ºr die Aktivierungen n√∂tige Speicher pro Rechenknoten um ca. 40% reduziert werden kann. Bei der Vorw√§rtsberechnung wird vor dem Aufmerksamkeitsmechanismus eine AllGather-Operation entlang der Sequenzdimension durchgef√ºhrt ($f$ in der Grafik in [Tensorparallelisierung](#tensor-parallelisierung)). Bei der R√ºckw√§rtsberechnung ist an dieser Stelle eine ReduceScatter-Operation notwendig. Da AllReduce-Operationen h√§ufig als AllGather und ReduceScatter implementiert werden, entspricht das Kommunikationsvolumen dem der Tensorparallelisierung nach @shoeybiMegatronLMTrainingMultiBillion2020. In der Praxis hat sich die Kombination von spaltenweiser Tensor-Parallelisierung mit dieser Form der Sequenz-Parallelisierung bew√§hrt, da sie eine kompromisslose Verringerung es n√∂tigen Speichervolumens erm√∂glicht.

@jacobsDeepSpeedUlyssesSystem2023 merken an, dass die Kommunikationskosten der Sequenz-Parallelisierung nach @liSequenceParallelismLong2022 und @korthikantiReducingActivationRecomputation2022 linear mit der Gr√∂√üe der Aktivierungen anw√§chst, was die Skalierung beeintr√§chtigt. Bei DeepSpeed Ulysses werden die Aktivierungen im gesamten Modell mit Ausnahme des Aufmerksamkeitsmoduls entlang der Sequenz-Dimension verteilt. Im Aufmerksamkeitsmodul werden die Aktivierungen hingegen entlang der Hidden-Dimension verteilt, wodurch die Aufmerksamkeitsk√∂pfe parallel berechnet werden k√∂nnen. Die Dimensionen der Aktivierungen werden an den Schnittstellen zum Aufmerksamkeitsmodul mittels All-to-All-Operationen neu verteilt. Da die Kommunikationskosten bei $D$ Prozessoren und einer Nachrichtengr√∂√üe von $M$ f√ºr ein All-to-All $\frac{M}{D}$ betragen, skaliert diese Methode um den Faktor $\frac{1}{D}$ besser als die oben genannten Methoden. Sie weisen experimentell eine starke Skalierung in der Sequenzl√§nge f√ºr ihre Methode nach. Ein weiterer Vorteil ist die Kompatibilit√§t dieser Methode mit verschiedenen Berechnungsverfahren f√ºr die Aufmerksamkeit.

## Hybride Parallelisierung

- 3D CNN hybrid parallelism [@oyamaCaseStrongScaling2021 üöß
- @narayananEfficientLargescaleLanguage2021 üöß
- @hagemannEfficientParallelizationLayouts2023 üöß

## Expertenparallelisierung üöß

Engl.: Expert parallelism, mixture of experts
- @huangHarderTasksNeed2024

## Automatische Parallelisierung

Das manuelle Design von optimalen Parallelisierungsstrategien kann vor allem bei inhomogenen Modellarchitekturen und Rechenclustern zeit- und arbeitsaufw√§ndig sein und erfordert √ºberdies viel Expertenwissen. Dies ist vor allem bei sich ver√§ndernden Gegebenheiten oder bei der Verwendung verschieden strukturierter Rechencluster schwer umsetzbar. Daher haben bereits verschiedene Arbeiten Algorithmen zur automatisierten Herleitung von Parallelisierungsstrategien vorgestellt [@jiaDataModelParallelism2018; @wangSupportingVeryLarge2019; @zhengAlpaAutomatingInter2022; @liuColossalAutoUnifiedAutomation2023; @wangImprovingAutomaticParallel2023].  

@jiaDataModelParallelism2018 @wangSupportingVeryLarge2019

Besonders hervorzuheben ist hierbei die Arbeit von @zhengAlpaAutomatingInter2022, welche mit Alpa ein System vorstellen, das auf Basis des statischen Rechengraphs eines Modells sowie der tats√§chlichen physischen Gegebenheiten des Rechenclusters eine nahezu optimale Parallelisierungsstrategie unter Einbezug von Inter-Operator- und Intra-Operator-Parallelisierung berechnet. Hierbei bezeichnet Inter-Operator-Parallelisierung das Zerteilen des Rechengraphs entlang der Kanten, w√§hrend bei der Intra-Operator-Parallelisierung die Operatoren, bzw. die Knoten, des Graphs selbst partitioniert werden. Inter-Operator-Parallelisierung ist damit eine Verallgemeinerung der [Pipeline-Parallelisierung](#pipeline-parallelisierung). Intra-Operator-Parallelisierung hingegen umfasst [Daten](#datenparallelisierung)-, [Tensor](#Tensor-Parallelisierung)- und [Sequenzparallelisierung](#datenparallelisierung), bei denen die einzelnen Elemente des Rechengraphs selbst, n√§mlich die Tensoren[^tensor] und Rechenoperationen, auf mehrere Rechenknoten verteilt werden.

Das Hauptproblem bei der automatischen Herleitung einer Parallelisierungsstrategie, die sowohl Inter- als auch Intra-Operator-Parallelisierung umfasst, ist der gewaltige Suchraum. Die Autoren von Alpa l√∂sen dieses Problem durch eine zweistufige Herangehensweise: F√ºr jede m√∂gliche Inter-Operator-Konfiguration suchen sie die (nahezu) optimale Intra-Operator-Konfiguration. Bei der Suche eines optimalen Intra-Operator-Parallelisierungsplans wird das Teilnetz des Clusters sowie die Partitionierung des Rechengraphs fest vorgegeben. Die Autoren formulieren die Berechnung des Intra-Operator-Plans als ein Integer-Linear-Programming-Problem, bei dem die Summe der Berechnungs-, Kommunikations- und Resharding-Kosten[^resharding] minimiert wird. Bei der Suche des Inter-Operator-Plans hingegen minimieren sie die maximale Latenz der gesamten Pipeline mittels Dynamic Programming.

Um die Komplexit√§t weiter zu reduzieren, wenden sie zudem einige Heuristiken an wie eine Einschr√§nkung der Submesh-Formen, die Vereinfachung des Rechengraphs, das Clustern von Operationen in Schichten mit √§hnlichem Rechenaufwand, oder das fr√ºhe Beenden der Suche bei Erreichen einer Maximallatenz.

[^resharding]: *Resharding* bezeichnet eine Neuverteilung bereits verteilter Eingabematrizen, die aufgrund aufeinander folgender Operatoren mit verschiedenen Verteilungsstrategien n√∂tig wird. Angenommen der Operator $o_{i-1}$ produziert eine Matrix, deren Reihen auf mehrere Rechenknoten verteilt sind, doch der Operator $o_i$ erwartet eine Matrix, deren Spalten auf mehrere Rechenknoten verteilt sind. Dann muss mittels einer [All-to-All](../communication-pattern/index.de.md)-Operation diese Matrix zun√§chst entsprechend der Eingabespezifikationen von $o_i$ neu verteilt werden.
[^tensor]: Eingabe- und Ausgabetensoren sowie die Parameter der Operatoren

@liuColossalAutoUnifiedAutomation2023 stellen mit ColossalAuto eine Strategie zur automatisierten Planung von Intra-Operator-Parallelisierung mit Activation Checkpointing vor. Sie bauen hierbei auf die Arbeit von Alpa auf, jedoch betrachten sie Inter-Operator-Parallelisierung nicht.

- Galvatron-BMW üöß
- √úbersicht üöß
- PipeDream [@narayananPipeDreamGeneralizedPipeline2019]: Planner f√ºr PP üöß
- DAPPLE [@fanDAPPLEPipelinedData2021]: Planner f√ºr PP üöß

## TODO

- Optimierungsrichtingen, z.B. kommunikation verringern ode verstecken, verteilung f√ºr bessere auslastung, lange sequenzen, ...

## Referenzen

{{bibliography}}
[^8]: 
