---
title: Parallelisierung des Maschinellen Lernens
description: Dieser Artikel beschreibt verschiedene Ans√§tze zur Parallelisierung von Trainings-Algorithmen im Maschinellen Lernen.
date: 2024-01-28 09:04:28+0800
image: cover.jpg
toc: true
math: true
draft: true
categories:
   - distributed-computing
   - machine-learning
weight: 1
---

# Parallelisierung des Maschinellen Lernens

J√ºngste Forschungen √ºber skalierbare Architekturen wie den Transformer [@vaswaniAttentionAllYou2017] haben gezeigt, dass die Leistungsf√§higkeit von neuronalen Netzen mit der Anzahl ihrer Parameter korreliert [@kaplanScalingLawsNeural2020] und dabei mit besonders gro√üen Netzwerken beeindruckende Ergebnisse erzielt [@openaiIntroducingChatGPT2022; @openaiGPT4TechnicalReport2023]. Dies f√ºhrte j√ºngst zu einem Wettlauf um die gr√∂√üten Modelle und damit verbunden einem drastischen Anstieg der Anforderungen an die Trainings- und Inferenz-Hardware. Diese Architekturen lassen sich zwar quasi beliebig vergr√∂√üern, doch sind der Skalierung in der Praxis durch die Hardware Grenzen gesetzt [@narayananEfficientLargescaleLanguage2021]. So hat sich beispielsweise die Zahl der Parameter der gr√∂√üten Modelle von 2018 bis 2021 vertausendfacht, w√§hrend die Speicherkapazit√§t der popul√§rsten Beschleuniger, Nvidia-Grafikkarten, sich in demselben Zeitraum lediglich verdoppelt hat. Mit der Gr√∂√üe der neuronalen Netze steigt auch die Trainingszeit und die Anzahl der f√ºr das Training ben√∂tigten Daten stetig an. Um also das Training immer gr√∂√üerer Modelle zu erm√∂glichen, ist die Verteilung der Rechenleistung auf mehrere Rechenknoten unumg√§nglich.

![Ein Liniendiagramm, das die Gr√∂√üe der neuronalen Netze in den Jahren von 2018 bis 2021 darstellt. Es sind die folgenden KI-Modelle eingezeichnet: 2018: ELMo (94 Mio.), BERT-L (340 Mio.); 2019: GPT-2 (1,5 Mrd.), Megatron-LM (8,3 Mrd.); 2020: Turing-NLG (17,2 Mrd.), GPT-3 (175 Mrd.). Alle diese Modelle liegen auf einer Geraden in einem logarithmischen Ma√üstab.](img/neural-network-size.png)
: Gr√∂√üenentwicklung von neuronalen Netzen √ºber die Zeit. Im Zeitraum von 2018 bis 2021 ist die Anzahl der Parameter in modernen neuronalen Netzen exponentiell angestiegen. Quelle: [@narayananEfficientLargescaleLanguage2021].

Dieser Artikel soll eine Einf√ºhrung in das verteilte Rechnen mit dem Fokus auf das maschinellen Lernen bieten. Zun√§chst werden die Grundlagen erl√§utert, anschlie√üend werden mit Daten- und Modellparallelisierung sowie Mixture of Experts verschiedene Formen der Parallelisierung eingef√ºhrt, die in der j√ºngsten Literatur diskutiert werden und in der Praxis bereits rege Anwendung finden.

## Grundlagen

Das Training immer gr√∂√üerer neuronaler Netze stellt die Entwickler vor zwei grunds√§tzliche Probleme: 

1. die f√ºr das Training n√∂tige Rechenzeit steigt mit der Anzahl der zu trainierenden Parameter an,
2. der Speicher aktueller Grafikkarten ist zu klein f√ºr die gro√üen Modelle inklusive ihrer Aktivierungen und Gradienten.

Diese Probleme k√∂nnen jeweils durch die Verteilung des Trainings auf mehrere Rechenknoten gel√∂st werden. Das Training von Modellen der Gr√∂√üe von ChatGPT und GPT-4 wird durch Parallelisierung √ºberhaupt erst m√∂glich gemacht. Doch f√ºhrt diese Parallelisierung zu einem weiteren Problem:

3. der durch Kommunikation und Synchronisation der Rechenknoten entstehende Mehraufwand steigt mit der Anzahl der Rechenknoten an.

### Scale Out Vs. Scale Up üöß

- Scale Up nur begrenzt m√∂glich, scale out erlaubt in der Theorie beliebige Skalierungsfaktoren (siehe [unten](#skalierbarkeit)).
- Effizienzsteigerung eine Form von Scle UP?

### Skalierbarkeit

- Ahmdahl: strong scalingüöß
- Gustavson: weak scalingüöß

Soll durch eine Parallelisierung die Rechenzeit verk√ºrzt werden, ist eine Betrachtung des Skalierungsverhaltens des Programms vonn√∂ten. Nicht jedes Programm profitiert gleicherma√üen von einer Verteilung auf mehrere Prozesse. Die Beschleunigung $S_p$ (engl.: *Speedup*), die durch die Verteilung auf $p$ Prozesse erreicht wird, ist das Verh√§ltnis zwischen der Rechenzeit[^Latenz] eines einzelnen Prozesses $T_1$ zur Rechenzeit von $p$ Prozessen $T_p$:

$$
S_p = \frac{T_1}{T_p}
$$
Auf Basis der Beschleunigung kann die Effizienz $E_p$ als das Verh√§ltnis der Beschleunigung $S_p$ zur Anzahl der Prozesse $p$ berechnet werden: 

$$
E_p = \frac{S_p}{p} = \frac{T_1}{T_p \cdot p}
$$
Die Effizienz (engl.: *efficiency*) gibt an, inwiefern das betrachtete Programm durch die Verteilung auf $n$ Prozesse profitiert. Bei einer Effizienz von $E_p = 1$ skaliert das Programm *linear*, also eine Erh√∂hung der Rechenkerne um einen Faktor $f$ f√ºhrt zu einer Erh√∂hung der Beschleunigung um den gleichen Faktor. Eine Effizienz von $E_p < 1$ deutet auf ein sub-lineares Skalierungsverhalten hin und eine Effizienz von $E_p > 1$ auf eine super-lineare Skalierung. Das folgende Schaubild verdeutlicht diese Beziehung:

![Speedup](img/scalability.svg)
: Verh√§ltnis der Beschleunigung (Speedup, $S_P$) zur Anzahl der parallelen Prozesse (\#Workers, $P$): Entspricht die Beschleunigung der Erh√∂hung der parallelen Prozesse ($S_P = P$, blaue Linie), skaliert das Programm linear; ist die Beschleunigung gr√∂√üer als die Anzahl der Prozesse ($S_P > P$, gr√ºner Bereich), liegt eine super-lineare Skalierung vor und im umgekehrten Fall ($S_P < P$, roter Bereich) skaliert das Programm sub-linear.

Zwar gibt es einige Algorithmen, die eine linear oder sogar super-linear skalieren [@aklSuperlinearPerformanceRealTime2004;@ristovSuperlinearSpeedupHPC2016], doch ist in der Praxis eine sub-lineare Skalierung aufgrund durch die Parallelisierung zus√§tzlich anfallender Aufgaben wie Kommunikation und Synchronisation zwischen den Prozessen die Regel [@mccoolStructuredParallelPrograming2012].

In der Forschung zur Parallelisierung von neuronalen Netzen wird h√§ufig nicht die Rechenzeit als Basis zur Berechnung der Beschleunigung herangezogen, da sich aufgrund der i.d.R. notwendigen Gradientenakkumulierung eine super-lineare Beschleunigung nur schwer erreichen l√§sst. Beispielsweise berechnen [@rajbhandariZeROMemoryOptimizations2020] die Beschleunigung in Bezug auf die Anzahl der Flie√ükommazahloperationen pro Sekunde (FLOPs).  üöß

[^Latenz]: Der Fachbegriff f√ºr die Rechenzeit ist "Latenz" (engl.: *latency*). 

### Klassifikation von Rechnerarchitekturenüöß

Flynn's taxonomy:

- SISD
- MISD
- SPMD/SIMD/SIMT
- MIMD/MPMD

### Rechnerhardware und Infrastruktur

Matrix-Operationen bilden die Grundlage f√ºr das Rechnen mit neuronalen Netzen. Diese lassen sich i.d.R. effizient parallelisieren, wodurch GPUs aufgrund ihrer tausenden von spezialisierten Prozessoren geeigneter f√ºr das Rechnen mit neuronalen Netzen sind als CPUs. Moderne Rechencluster setzen daher haupts√§chlich auf GPUs oder speziell f√ºr die Anwendung im maschinellen Lernen entwickelte Neural Processing Units (NPUs). Beispiele f√ºr NPUs sind Googles TPU [@jouppiInDatacenterPerformanceAnalysis2017a; @googleSystemArchitectureCloud2023] und Huaweis Ascend-Plattform [@huaweiAscendComputing2024].

Da der Speicher einzelner GPUs begrenzt ist, aber die Gr√∂√üe moderner neuronaler Netze exponentiell anw√§chst [@narayananEfficientLargescaleLanguage2021], kommen in der k√ºnstlichen Intelligenz vermehrt verteilte Architekturen zum Einsatz.

Ein auf GPUs basierendes Cluster besteht aus mehreren Racks mit mittels Ethernet oder InfiniBand √ºber Netzwerkswitches verbundenen Rechnern, an welchen wiederum √ºber PCIe oder herstellerspezifische Sockets wie SXM mehrere GPUs angeschlossen sind. Die GPUs wiederum k√∂nnen mittels bandbreitenstarker Direktverbindungen und spezialisierter Switches zus√§tzlich untereinander vernetzt werden, um h√∂here Bandbreiten bei √úbertragungen zwischen den GPUs eines Rechners zu erm√∂glichen[^NVLink]. Die Verbindungen in den unteren Schichten dieses Baumes haben i.d.R. die h√∂chste Bandbreite, w√§hrend in den dar√ºber liegenden Schichten die Bandbreite weiter abnimmt.

![Topologie eines GPU-Clusters: Vier Server sind √ºber einen Switch mittels d√ºnner Linien verbunden, unterhalb der Server sind jeweils vier GPUs abgebildet, die mit dem Server √ºber dickere Linien verbunden sind. Zus√§tzlich sind die vier GPUs jedes Servers √ºber sehr dicke Linien untereinander vernetzt.](img/gpu-cluster-topology.svg)
: Die Netzwerktopologie eines GPU-Clusters. Die Dicke der Verbindungslinien zwischen den Komponenten ist proportional zu deren Bandbreite.

- SuperPOD üöß (xPod?) https://docs.nvidia.com/https://docs.nvidia.com/dgx-superpod-reference-architecture-dgx-h100.pdf

Im Gegensatz zur hierarchischen Topologie in GPU-Clustern sind TPUs in einer 2D-Matrix angeordnet. Jede TPU ist selbst netzwerkf√§hig und √ºber schnelle Direktverbindungen mit jeweils vier anderen TPUs verbunden. Insgesamt bilden die TPUs einen Torus, was u.a. effiziente All-Reduce-Operationen erm√∂glicht [@jouppiDomainspecificSupercomputerTraining2020].

![Topologie eines TPU-Clusters: Die TPUs sind in einer 2D-Matrix angeordnet und sind horizontal und vertikal miteinander verbunden. Die TPUs an den R√§ndern der Matrix sind mit den jeweiligen TPUs an den anderen Enden verbunden, sodass die Struktur insgesamt einen Torus bildet.](img/tpu-cluster-topology.svg)
: Die Netzwerktopologie eines TPU-Clusters. Die Farben dienen lediglich der besseren Unterscheidung der Verbindungen.

[^NVLink]: NVIDIA bietet diesbez√ºglich NVLink und NVSwitch an [@nvidiaNVLinkNVSwitchFastest2023].

## Ans√§tze

Inspiriert durch die Arbeit von @zhengAlpaAutomatingInter2022 verwende ich in diesem Artikel eine ganzheitliche Sicht auf die Parallelisierung im maschinellen Lernen. Jedes ML-Modell kann in Form eines Graphs dargestellt werden, dessen Knoten entweder Daten (Eingabedaten, Parameter oder Aktivierungen) oder Operationen (Addition, Multiplikation, ...) darstellen. Bei der Verteilung eines Modells werden Teile des Graphs entlang einer oder mehrerer Dimensionen partitioniert und auf verschiedene Ger√§te verteilt. Die Operationen werden schlie√ülich auf allen Ger√§ten ausgef√ºhrt, auf denen ein Teil der Eingabedaten liegt. In der Literatur wird in Abh√§ngigkeit von den partitionierten Daten in Datenparallelisierung, Tensor-Parallelisierung und Pipeline-Parallelisierung unterschieden:

![Es sind vier Graphen zu sehen.](img/parallelism-overview.svg)

: √úbersicht √ºber verschiedene g√§ngige Formen der Parallelisierung: Ein Rechengraph (a) kann auf verschiedene Arten verteilt werden. Bei der Datenparallelisierung (b) werden die Daten auf mehrere Knoten verteilt, bei der Tensor-Parallelisierung (c) die Modellparameter und bei der Pipeline-Parallelisierung (d) der Graph selbst. Gegebenenfalls m√ºssen zus√§tzlich Kommunikationsoperationen durchgef√ºhrt werden.

Bei der Datenparallelisierung werden die Eingabedaten sowie Aktivierungen auf mehrere Ger√§te verteilt, bei der Tensor-Parallelisierung hingegen die Modellparameter und Aktivierungen, w√§hrend bei der Pipeline-Parallelisierung ganze Teile des Graphs verteilt werden. Diese Formen der Parallelisierung werden im Folgenden n√§her erl√§utert. Obwohl sich diese Formen der Parallelisierung gleicherma√üen auf die Inference anwenden lassen, liegt der Fokus in den folgenden Ausf√ºhrungen auf das Training, da der Inference-Prozess der Vorw√§rtsberechnung beim Training entspricht.

- welche Ans√§tze erm√∂glichen strong/weak scaling?üöß

## Datenparallelisierung

Die Datenparallelislierung ist die einfachste und am weitesten verbreitete Form der Parallelisierung im maschinellen Lernen. Beim der Datenparallelisierung werden die Eingabedaten auf mehrere Rechenknoten verteilt, was das Training auf gr√∂√üeren Datens√§tzen erm√∂glicht[^spmd]. Jeder Rechenknoten beh√§lt dabei das vollst√§ndige Modell im Speicher, wodurch die Gr√∂√üe des Modells weiterhin durch den verf√ºgbaren Speicher begrenzt wird [@rajbhandariZeROMemoryOptimizations2020]. Jeder Knoten berechnet unabh√§ngig voneinander die Ausgabe und Gradienten und synchronisiert anschlie√üend die lokalen Gradienten mit den √ºbrigen Knoten mittels einer [All-Reduce](/blog/de/p/kommunikationsmuster/#all-reduce)-Operation. Abschlie√üend aktualisiert jeder Knoten seine lokalen Modellparameter auf Basis der akkumulierten Gradienten. In der nachfolgenden Graphik werden die Eingabedaten entlang der Batch-Dimension auf die verschiedenen Rechenknoten verteilt. Jeder Rechenknoten berechnet anschlie√üend unabh√§ngig voneinander die Ausgabe.

[^spmd]: Datenparallelisierung kann auch als "[Single Program Multiple Data](https://de.wikipedia.org/wiki/Single-Program_Multiple-Data)" (SPMD) betrachtet werden, wobei im maschinellen Lernen das KI-Modell mit seinen Instruktionen und Modellparametern das ausgef√ºhrte Programm darstellt.

![SVG image](img/data-parallel.svg)
: Datenparalleles Training von neuronalen Netzen. Das Modell wird auf $n$ Knoten repliziert, w√§hrend die Trainingsdaten gleichm√§√üig auf die Rechenknoten verteilt werden. Hier werden die Daten entlang der Batch-Dimension verteilt.

Alternativ zur All-Reduce-Operation k√∂nnen die Modellparameter durch einen Parameter-Server aktualisiert werden [@liScalingDistributedMachine2014]. Die Aufgabe des Parameter-Servers ist die zentrale Aktualisierung der Modell-Parameter auf Basis der Gradienten aller teilnehmenden Prozesse. Da allerdings der Server mit allen Prozessen kommunizieren muss, skaliert er nur begrenzt. In der Praxis wird dieser Ansatz aktuell selten gew√§hlt.

√úblicherweise werden die Daten entlang der Batch-Dimension verteilt, doch k√∂nnen die Daten auch entlang der anderen Dimensionen (z.B. der Sequenzl√§nge) partitioniert werden. In einigen Dom√§nen wie z.B. der nat√ºrlichen Sprachverarbeitung k√∂nnen die Eingabedaten sehr lange Sequenzen enthalten, wie z.B. lange Dokumente oder Audiodaten, was bei einigen Modellarchitekturen zu Speicherproblemen f√ºhrt. So hat beispielsweise der Transformer [@vaswaniAttentionAllYou2017] eine quadratische Zeit- und Speicherkomplexit√§t in Bezug auf die Eingabel√§nge [@childGeneratingLongSequences2019]. @liSequenceParallelismLong2022 f√ºhren daher die Sequenzparallelisierung ein, bei der die Sprachdaten anhand der L√§ngendimension partitioniert werden. Hierdurch wird das Training auf Datens√§tzen mit z.T. langen Eingabedokumenten erm√∂glicht. Es sind jedoch Anpassungen f√ºr Modellarchitekturen notwendig, die das gesamte Dokument betrachten. Im Falle des Transformers, der die √Ñhnlichkeiist er nur begrenzt skalierbarist er nur begrenzt skalierbart jedes Tokens im Eingabedokument mit jedem anderen Token berechnet, schlagen die Autoren *Ring Self Attention* vor, eine Variante des Aufmerksamkeitsmechanismus [@vaswaniAttentionAllYou2017], bei dem die partitionierten Key- und Value-Embeddings zwischen allen Rechenknoten zirkulieren. @jacobsDeepSpeedUlyssesSystem2023 hingegen implementieren in DeepSpeed Ulysses eine andere Form der Sequenzparallelisierung: TODO üöß

Da beim datenparallelen Training alle Rechenknoten alle Modellparameter mitsamt der zugeh√∂rigen Gradienten und Optimierer-Zust√§nden speichern m√ºssen, ist die Modellgr√∂√üe durch den verf√ºgbaren Speicher der einzelnen Knoten begrenzt. @rajbhandariZeROMemoryOptimizations2020 stellen deshalb den *Zero Redundancy Optimizer* (ZeRO) vor, der durch die gleichm√§√üige Verteilung der Optimierer-Zust√§nde, Gradienten und Modellparameter auf alle Rechenknoten den Speicherverbrauch drastisch reduziert. Werden nur die Optimierer-Zust√§nde und Gradienten verteilt, entspricht das Kommunikationsvolumen von ZeRO dem des datenparallelen Trainings. Bei zus√§tzlicher Partitionierung der Modellparameter erh√∂ht es sich um 50%[^zeropp]. Die Autoren weisen in Experimenten nach, dass das datenparallele Training mit diesem Optimierer in Bezug auf FLOPs super-linear skaliert.

[^zeropp]: In der Praxis kann diese Erh√∂hung des Kommunikationsvolumens einen nicht-vernachl√§ssigbaren Einfluss auf den Trainingsdurchsatz haben. Daher kombinieren @wangZeROExtremelyEfficient2023 Quantisierung mit einer neuen Platzierungsstrategie, um den Overhead durch die Kommunikation zu reduzieren.

PyTorch unterst√ºtzt mit PyTorch DDP [@liPyTorchDistributedExperiences2020a] schon seit l√§ngerem datenparalleles Training nativ. Die Unterst√ºtzung f√ºr die zus√§tzliche Partitionierung von Optimierer-Zust√§nden, Gradienten und Modellparametern wurde mit PyTorch FSDP [@zhaoPyTorchFSDPExperiences2023] in PyTorch 2.0 eingef√ºhrt.

Sequence-level parallelism for distributed training of long context transformers?? üöß

## Modellparallelisierung

Bei der Modellparallelisierung werden die Parameter des Modells auf verschiedene Rechenknoten verteilt. Es wird hierbei zwischen Tensor- und Pipeline-Parallelisierung unterschieden.

### Pipeline-Parallelisierung

Bei der Pipeline-Parallelisierung (PP) [@huangGPipeEfficientTraining2019; @narayananPipeDreamGeneralizedPipeline2019; @fanDAPPLEPipelinedData2020] wird das Modell in Abschnitte aufeinander folgender Operatoren zerteilt und diese Abschnitte verschiedenen Rechenknoten zugeordnet. Die Aktivierungen der einzelnen Abschnitte werden mittels Punkt-zu-Punkt-Kommunikation[^sendrecv] auf den Rechenknoten mit dem jeweils folgenden Abschnitt √ºbertragen. H√§ufig wird das Modell entlang der Schichtengrenzen zerteilt, da das Kommunikationsvolumen hier √ºblicherweise gering ist. Pipeline-Parallelisierung ist analog zu einem bidirektionalen Staffellauf.

[^sendrecv]: Die zugeh√∂rigen Kommunikationsprimitive werden *Send* (senden) und *Recv* (receive, empfangen) genannt.

![Visualisierung der einfachsten Form einer Pipeline-Parallelisierung](img/pp-naive.svg)
: Visualisierung der einfachsten Form einer Pipeline-Parallelisierung: Die Mini-Batches werden nacheinander in das Modell hinein gegeben. Zun√§chst berechnet der erste Prozess auf GPU 1 die Aktivierungen f√ºr die ersten Schichten und leitet sie an den zweiten Prozess weiter, welcher auf der zweiten GPU die Aktivierungen der nachfolgenden Schichten berechnet. Dieser Prozess wiederholt sich, bis die Vorw√§rtsberechnung f√ºr as gesamte Modell abgeschlossen wurde. Anschlie√üend erfolgt die R√ºckw√§rtsberechnung entsprechend in umgekehrter Reihenfolge. Nach Abschluss der R√ºckw√§rtsberechnung werden auf allen Ger√§ten parallel die Parameter der lokal verf√ºgbaren Schichten auf Basis der lokalen Gradienten aktualisiert. Danach kann die Berechnung des n√§chsten Mini-Batches gestartet werden.

Diese Form der Parallelisierung erm√∂glicht prinzipiell ein beliebiges Skalieren der Modelltiefe. Der Vorteil dabei ist das bei geeigneter Wahl der Abschnitte geringe Kommunikationsvolumen und ist damit auch f√ºr Netzwerke mit geringerer Bandbreite geeignet. Allerdings f√ºhrt die Hintereinanderausf√ºhrung der Abschnitte zu Phasen geringer Rechenlast, die sog. *bubble time*, bei denen nur ein Teil der reservierten Rechenknoten genutzt wird. Ein Hauptziel der Optimierung von pipeline-parallelen Setups ist daher die Minimierung der Bubble-Time.

Durch die Verwendung von Micro-Batches[^micro] [@huangGPipeEfficientTraining2019][^gpipe] kann die Bubble-Zeit verringert werden. Dabei werden die herk√∂mmlichen Mini-Batches in Micro-Batches zerteilt und nacheinander in das Modell eingegeben. Da die Micro-Batches Teil eines Mini-Batches sind, werden die Parameter erst aktualisiert, nachdem die Gradienten aller Micro-Batches vorliegen. Daher k√∂nnen die Rechenknoten gleichzeitig die Vorw√§rts- und R√ºckw√§rtsberechnung f√ºr verschiedene Micro-Batches durchf√ºhren. Die geringere Berechnungszeit von kleineren Batches und die √ºberlappende Bearbeitung der Micro-Batches durch verschiedene Rechenknoten f√ºhrt zu einer besseren Auslastung der Rechenknoten. Je h√∂her die Anzahl der Micro-Batches, desto geringer ist der Anteil der Idle-Zeit [@narayananEfficientLargescaleLanguage2021].

[^micro]: Micro-Batches sind konzeptionell √§quivalent zu Gradientenakkumulation.
[^gpipe]: GPipe

![](img/pp-microbatches.svg)
: Visualisierung der Ausf√ºhrung eines Modells (links) mittels Pipeline-Parallelisierung auf vier Beschleunigern. Der Mini-Batch wurde in acht Micro-Batches unterteilt, welche jeweils nacheinander in das Modell eingegeben werden. Nachdem die R√ºckw√§rtsberechnung des achten Micro-Batches beendet wurde, werden die Parameter auf Basis der berechneten Gradienten aktualisiert und die n√§chsten Micro-Batches in das Modell eingegeben.

Allerdings erh√∂ht sich dabei der f√ºr die Aktivierungen n√∂tige Speicheraufwand[^activations], denn f√ºr jeden Micro-Batch m√ºssen jeweils die Aktivierungen solange gespeichert werden, bis die Vorw√§rtsberechnung aller Micro-Batches abgeschlossen ist. Das f√ºhrt bei $B$ Micro-Batches zu einem maximalen Speicherverbrauch f√ºr Aktivierungen von $B \cdot M_a$. Der 1F1B-Schedule[^1f1b] [@narayananPipeDreamGeneralizedPipeline2019; @narayananEfficientLargescaleLanguage2021; @fanDAPPLEPipelinedData2021] l√∂st dieses Problem durch die unverz√ºgliche R√ºckw√§rtsberechnung eines Micro-Batches, sobald die Vorw√§rtsberechnung f√ºr alle $S$ Pipeline-Abschnitte abgeschlossen ist. Der 1F1B-Schedule operiert in zwei Phasen: In der Aufw√§rmphase nimmt jeder Knoten zun√§chst nur Vorw√§rtsberechnungen, anschlie√üend auch R√ºckw√§rtsberechnungen vor. Es kommt hierbei allerdings auch zu Idle-Zeiten. Nachdem die Vorw√§rtsberechnung der ersten $S$ Micro-Batches abgeschlossen ist, wechselt 1F1B in die Steady-State-Phase, in welcher jeder Knoten jeweils eine Vorw√§rts- und R√ºckw√§rtsberechnung abwechselnd ausf√ºhrt. Der f√ºr die Speicherung der Aktivierungen n√∂tige maximale Speicheraufwand reduziert sich daher auf $S \cdot M_a$, da zu jedem Zeitpunkt maximal $S$ Micro-Batches aktiv berechnet werden. Bei DAPPLE [@fanDAPPLEPipelinedData2021] und PipeDream-Flush [@narayananEfficientLargescaleLanguage2021] werden die Modell-Parameter aller Knoten gleichzeitig aktualisiert, sobald die R√ºckw√§rtsberechnung des Mini-Batches abgeschlossen ist (der sog. *Pipeline-Flush*). Hierbei schlie√üt auf die Steady-State-Phase eine Abk√ºhlungsphase an, in welcher die letzten R√ºckw√§rtsberechnungen abgeschlossen, aber keine neuen Vorw√§rtsberechnungen mehr vorgenommen werden. Die Idle-Zeit von 1F1B entspricht der von GPipe.

![](img/pp-1f1b.svg)

: 1F1B-Schedule mit synchroner Parameteraktualisierung nach DAPPLE [@fanDAPPLEPipelinedData2021] und PipeDream-Flush [@narayananEfficientLargescaleLanguage2021].

PipeDream [@narayananPipeDreamGeneralizedPipeline2019] erreicht neben der Speichereinsparung eine Reduktion der Idle-Zeit, indem die Parameteraktualisierung asynchron stattfindet und die Berechnung des n√§chsten Mini-Batches bereits beginnt, w√§hrend die R√ºckw√§rtsberechnung des aktuellen noch nicht abgeschlossen ist. Dadurch wird zwar die maximale Auslastung beim √úbergang zwischen Mini-Batches erreicht, doch muss f√ºr jeden Micro-Batch jeweils eine Version der Modellparameter vorgehalten werden. Bei PipeDream-2BW [@narayananEfficientLargescaleLanguage2021] wird die Anzahl der Modellparameter-Versionen auf zwei reduziert, indem die Aktualisierung der Parameter erst nach Abschluss des Mini-Batches erfolgt. Der asynchrone Schedule ist allerdings nicht mathematisch √§quivalent zu SGD und dessen Konvergenz daher theoretisch nicht garantiert. Nichtsdestotrotz k√∂nnen @narayananEfficientLargescaleLanguage2021 nachweisen, dass sich die asynchrone Parameteraktualisierung nicht nennenswert auf das Konvergenzverhalten auswirkt.

![Async 1F1B](img/pp-1f1b-async.svg)

: PipeDream-2BW: Asynchroner 1F1B-Schedule mit double buffered weights.

[^1f1b]: 1F1B steht f√ºr "*one forward one backward*"
[^activations]: Die Anzahl der n√∂tigen Aktivierungen kann zwar mittels *Activation Checkpointing* reduziert werden, doch m√ºssen die Eingabe-Aktivierungen jeder Modell-Partition weiterhin gespeichert werden. Daher bleibt der Speicheraufwand proportional zur Anzahl der Micro-Batches.

Der verschachtelte (*interleaved*) 1F1B-Schedule [@narayananEfficientLargescaleLanguage2021] ist eine Optimierung des 1F1B-Schedules mit reduzierter Idle-Zeit. Diese Reduktion wird dadurch erreicht, dass ein Rechenknoten f√ºr mehrere nicht aufeinander folgende Partitionen des Modells zust√§ndig ist (*model chunks*). Da die Partitionen des Modells nun kleiner sind, k√∂nnen sie schneller berechnet werden. Damit k√∂nnen die Micro-Batches schneller in die Pipelines eingegeben werden. Bei $v$ Modellpartitionen verringert sich die Bubble-Zeit um den Faktor $v$.

![](img/pp-1f1b-interleaved.svg)

@liChimeraEfficientlyTraining2021 erreichen mit Chimera eine bessere Auslastung durch die Verwendung mehrerer paralleler Pipelines. Diese Pipelines verwenden die verf√ºgbaren Rechenknoten in jeweils entgegengesetzter Richtung. Dies f√ºhrt zwar zu einer Verringerung der Bubble-Time, allerdings ist pro Pipeline jeweils eine Kopie der Modellparameter n√∂tig Chimera kann als Pipeline-Parallelisierung mit Datenparallelisierung auf Ebene der Micro-Batches betrachtet werden. Daher ist neben der P2P-Kommunikation zus√§tzlich ein All-Reduce zwischen Knoten mit den gleichen Modellschichten n√∂tig, um nach Abschluss der R√ºckw√§rtsberechnung die Gradienten zu synchronisieren. Diese zus√§tzlichen All-Reduce-Operationen k√∂nnen allerdings in den Bubbles am Ende der Mini-Batch-Berechnung eingef√ºgt werden.

![Chimera](img/pp-chimera.svg)

: Chimera mit zwei Pipelines

Bei der R√ºckw√§rtsberechnung werden die Ableitungen des Fehlers in Bezug auf die Aktivierungen und Parameter berechnet, doch w√§hrend erstere zur R√ºckw√§rtsberechnung in vorherigen Schichten ben√∂tigt werden, werden letztere erst bei der Parameteraktualisierung ben√∂tigt (siehe nachfolgende Grafik). Diesen Umstand machen sich @qiZeroBubblePipeline2023 zunutze und zerlegen die R√ºckw√§rtsberechnung f√ºr einen Micro-Batch in zwei Schritte: der R√ºckw√§rtsberechnung f√ºr die Aktivierungen, welche m√∂glichst fr√ºhzeitig vollzogen werden sollte, und die R√ºckw√§rtsberechnung f√ºr die Parameter, welche zu einem beliebigen Zeitpunkt zwischen der R√ºckw√§rtsberechnung f√ºr die Aktivierungen und der Parameteraktualisierung stattfinden kann. Damit kann die R√ºckw√§rtsberechnung der vorherigen Schicht bereits beginnen, bevor die komplette R√ºckw√§rtsberechnung abgeschlossen ist.

![](img/backward-graph.svg)

: Visualisierung der Vorw√§rts- und R√ºckw√§rtsberechnung. Bei der R√ºckw√§rtsberechnung werden jeweils die Ableitungen des Fehlers $E$ in Bezug auf die Parameter $\theta$ und die Aktivierungen $x$ berechnet. Dabei f√§llt auf, dass nur die Ableitungen in Bezug auf die Aktivierungen w√§hrend der R√ºckw√§rtsberechnung ben√∂tigt werden. Die Ableitungen bez√ºglich der Parameter (Gradienten) werden erst bei der Parameteraktualisierung ben√∂tigt.

Dadurch schaffen es die Autoren einen Pipeline-Schedule mit synchronen Parameterupdates zu kreieren, der in der Theorie keine Idle-Zeit beim √úbergang zwischen den Mini-Batches aufweist. Durch die zus√§tzliche Kombination mit einem verschachtelten Schedule[^zero-pp-interleaved], dessen zweiter Teil entgegengesetzt zum ersten Teil verl√§uftüöß, stellen sie zudem sicher, dass die nur w√§hrend der R√ºckw√§rtsberechnung ben√∂tigten Ableitungen bzgl. der Aktivierungen schneller wieder entfernt werden k√∂nnen[^zero-pp-aktiv]. In ihren Experimenten zeigen die Autoren, dass ihr Schedule einen bis zu XX% besseren Durchsatz als 1F1B erreicht.

[^zero-pp-interleaved]: Siehe *interleaved 1F1B*
[^zero-pp-aktiv]: Die erste Iteration des bubble-freien Pipeline-Schedules erforderte einen Speicher von $O(2 p M_a)$, was einen Flaschenhals darstellen kann.

![](img/pp-zero-bubble.svg)

- Token-level parallelism: makes good use of the property of the transformer that long sequences require a longer time to compute [@liangSurveyAutoParallelismLargeScale2023]. Instead of feeding data in the unit of micro-batch to the pipeline, terapipe [@liTeraPipeTokenLevelPipeline2021] splits sequence data along the token axis (i.e., length axis) unevenly and then feeds them in the pipeline where each spit has a similar execution time, orthogonal to TP, helpful in large-scale language models
- Optimize communication: Multiple streams to send/recv [@jiangMegaScaleScalingLarge2024]
- Modellpartitionierung: M√∂glichst balanciert (Time, memory), geringes Kommunikationsvolumen (Aktivierungen)
- √úbersicht: BUbble-Time, Speicher, Kommunikation, async [@liChimeraEfficientlyTraining2021]

### Tensor-Parallelisierung

Bei der Tensor-Parallelisierung werden die Parameter-Tensoren des Modells partitioniert und auf die verschiedenen Rechenknoten verteilt. Gegebenenfalls muss auf die Operation eine Reduktion folgen, um das abschlie√üende Ergebnis zu erhalten. Beispielsweise liegt bei einer verteilten Matrixmultiplikation das Ergebnis erst nach einem All-Reduce vollst√§ndig vor.

Megatron-LM [@shoeybiMegatronLMTrainingMultiBillion2020] partitioniert die Parameter der Matrixmultiplikationen im Self-Attention-Block sowie der MLPs spaltenweise. Diese Matrizen ben√∂tigen vor allem bei Transformern sehr viel Speicher (quadratisch in der Sequenzl√§nge), sodass deren Verteilung auf verschiedene Rechenknoten sehr viel Speicher einsparen kann. Allerdings ist durch die nachfolgende Synchronisation das Kommunikationsvolumen bei dieser Form der Parallelisierung sehr hoch, weshalb sie hohe Bandbreiten zwischen den Rechenknoten verlangt, auf denen die Operation ausgef√ºhrt wird. Aufgrund des hierarchischen Aufbaus von GPU-Clustern mit bandbreitenstarken Direktverbindungen zwischen den einzelnen GPUs eines einzelnen Rechners und den vergleichsweise langsamen Verbindungen zwischen verschiedenen Rechnern, ist diese Form der Parallelisierung √ºblicherweise am geeignetsten f√ºr die Anwendung zwischen den GPUs eines einzelnen Rechners, deren Bandbreite sie voll ausnutzen kann [@shoeybiMegatronLMTrainingMultiBillion2020].

Grafik: Matrixmultiplikation üöß

Neben den eindimensionalen spaltenweisen Partitionierungen der Matrixmultiplikationen in Megatron-LM wurden auch andere Arten der Partitionierung vorgestellt, wie z.B. die reihenweise Partitionierung üöß oder die mehrdimensionale Partitionierung (2D, 2,5D, 3D) [@xuEfficient2DMethod2021; @bianMaximizingParallelismDistributed2021; @liColossalAIUnifiedDeep2022]. Obwohl sich bei der mehrdimensionalen Partitionierung in der Summe das Kommunikationsvolumen erh√∂ht, skaliert sie besser als die eindimensionale Partitionierung, da bei der eindimensionalen Partitionierung ein All-Reduce √ºber alle Knoten vonn√∂ten ist, w√§hrend sich bei der h√∂her-dimensionalen Partitionierung durch mehrere voneinander unabh√§ngige All-Reduce-Operationen eine geringere Latenz erreichen l√§sst üöß.

Grafik: nd-Partitionierung üöß

@korthikantiReducingActivationRecomputation2022 schlagen neben der Partitionierung der Matrizen der Matrixmultiplikationen des Transformers zus√§tzlich die Parallelisierung der LayerNorm- und Dropout-Operatoren vor. Obwohl die Parameter dieser Operatoren im Vergleich zu den Parametern des Aufmerksamkeitsmechanismus und MLPs relativ klein sind, kann durch deren Verteilung der f√ºr die Aktivierungen n√∂tiger Speicher merklich reduziert werden. Da im Gegensatz zu den oben genannten Matrixoperationen diese Operationen reihenweise partitioniert werden üöß, ist ein zus√§tzlicher Umformungsschritt n√∂tig, um die Daten an den Schnittstellen zum Aufmerksamkeits- und MLP-Modul korrekt umzuwandeln (üöß welche Primitive?). Sie nennen diese Form der Parallelisierung Sequenzparallelisierung, doch hat diese Methode au√üer dem Namen keinerlei Gemeinsamkeiten mit der von @liSequenceParallelismLong2022 vorgestellten Sequenzparallelisierung. In der Praxis hat sich die Kombination von spaltenweiser Tensor-Parallelisierung mit Sequenzparallelisierung bew√§hrt, da sie zueinander komplement√§r sind und der geringere Speicherverbrauch entweder gr√∂√üere Batch-Gr√∂√üen erm√∂glicht, die wiederum den Durchsatz erh√∂hen, oder das Training √ºberhaupt erst m√∂glich machen. Zudem l√§sst sich der durch die zus√§tzlichen Umformungsschritte entstehende Overhead durch geschickte Verschmelzung mit den nachfolgenden Operationen deutlich reduzieren.üöß

- make full use of bandwidth, communication brought by segmentation is basically efficient collective commüöß
- SUMMA @vandegeijnSUMMAScalableUniversal1997, 3D matmul @agarwalThreedimensionalApproachParallel1995

## Hybride Parallelisierung

- 3D CNN hybrid parallelism [@oyamaCaseStrongScaling2021 üöß

## Expertenparallelisierung üöß

Engl.: Expert parallelism, mixture of experts
- @huangHarderTasksNeed2024

## Automatische Parallelisierung

Das manuelle Design von optimalen Parallelisierungsstrategien kann vor allem bei inhomogenen Modellarchitekturen und Rechenclustern zeit- und arbeitsaufw√§ndig sein und erfordert √ºberdies viel Expertenwissen. Dies ist vor allem bei sich ver√§ndernden Gegebenheiten oder bei der Verwendung verschieden strukturierter Rechencluster schwer umsetzbar. Daher haben bereits verschiedene Arbeiten Algorithmen zur automatisierten Herleitung von Parallelisierungsstrategien vorgestellt [@jiaDataModelParallelism2018; @wangSupportingVeryLarge2019; @zhengAlpaAutomatingInter2022; @liuColossalAutoUnifiedAutomation2023; @wangImprovingAutomaticParallel2023].  

@jiaDataModelParallelism2018 @wangSupportingVeryLarge2019

Besonders hervorzuheben ist hierbei die Arbeit von @zhengAlpaAutomatingInter2022, welche mit Alpa ein System vorstellen, das auf Basis des statischen Rechengraphs eines Modells sowie der tats√§chlichen physischen Gegebenheiten des Rechenclusters eine nahezu optimale Parallelisierungsstrategie unter Einbezug von Inter-Operator- und Intra-Operator-Parallelisierung berechnet. Hierbei bezeichnet Inter-Operator-Parallelisierung das Zerteilen des Rechengraphs entlang der Kanten, w√§hrend bei der Intra-Operator-Parallelisierung die Operatoren, bzw. die Knoten, des Graphs selbst partitioniert werden. Inter-Operator-Parallelisierung ist damit eine Verallgemeinerung der [Pipeline-Parallelisierung](#pipeline-parallelisierung). Intra-Operator-Parallelisierung hingegen umfasst [Daten](#datenparallelisierung)-, [Tensor](#Tensor-Parallelisierung)- und [Sequenzparallelisierung](#datenparallelisierung), bei denen die einzelnen Elemente des Rechengraphs selbst, n√§mlich die Tensoren[^tensor] und Rechenoperationen, auf mehrere Rechenknoten verteilt werden.

Das Hauptproblem bei der automatischen Herleitung einer Parallelisierungsstrategie, die sowohl Inter- als auch Intra-Operator-Parallelisierung umfasst, ist der gewaltige Suchraum. Die Autoren von Alpa l√∂sen dieses Problem durch eine zweistufige Herangehensweise: F√ºr jede m√∂gliche Inter-Operator-Konfiguration suchen sie die (nahezu) optimale Intra-Operator-Konfiguration. Bei der Suche eines optimalen Intra-Operator-Parallelisierungsplans wird das Teilnetz des Clusters sowie die Partitionierung des Rechengraphs fest vorgegeben. Die Autoren formulieren die Berechnung des Intra-Operator-Plans als ein Integer-Linear-Programming-Problem, bei dem die Summe der Berechnungs-, Kommunikations- und Resharding-Kosten[^resharding] minimiert wird. Bei der Suche des Inter-Operator-Plans hingegen minimieren sie die maximale Latenz der gesamten Pipeline mittels Dynamic Programming.

Um die Komplexit√§t weiter zu reduzieren, wenden sie zudem einige Heuristiken an wie eine Einschr√§nkung der Submesh-Formen, die Vereinfachung des Rechengraphs, das Clustern von Operationen in Schichten mit √§hnlichem Rechenaufwand, oder das fr√ºhe Beenden der Suche bei Erreichen einer Maximallatenz.

[^resharding]: *Resharding* bezeichnet eine Neuverteilung bereits verteilter Eingabematrizen, die aufgrund aufeinander folgender Operatoren mit verschiedenen Verteilungsstrategien n√∂tig wird. Angenommen der Operator $o_{i-1}$ produziert eine Matrix, deren Reihen auf mehrere Rechenknoten verteilt sind, doch der Operator $o_i$ erwartet eine Matrix, deren Spalten auf mehrere Rechenknoten verteilt sind. Dann muss mittels einer [All-to-All](../communication-pattern/index.de.md)-Operation diese Matrix zun√§chst entsprechend der Eingabespezifikationen von $o_i$ neu verteilt werden.
[^tensor]: Eingabe- und Ausgabetensoren sowie die Parameter der Operatoren

@liuColossalAutoUnifiedAutomation2023 stellen mit ColossalAuto eine Strategie zur automatisierten Planung von Intra-Operator-Parallelisierung mit Activation Checkpointing vor. Sie bauen hierbei auf die Arbeit von Alpa auf, jedoch betrachten sie Inter-Operator-Parallelisierung nicht.

- Galvatron-BMW üöß
- √úbersicht üöß
- PipeDream [@narayananPipeDreamGeneralizedPipeline2019]: Planner f√ºr PP üöß
- DAPPLE [@fanDAPPLEPipelinedData2021]: Planner f√ºr PP üöß

## TODO

- Optimierungsrichtingen, z.B. kommunikation verringern ode verstecken, verteilung f√ºr bessere auslastung, lange sequenzen, ...

## Referenzen

{{bibliography}}
[^8]: 
