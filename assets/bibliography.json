[
  {"id":"leyrerBesserLebenMit2022","abstract":"Du \"benützt\" schon ssh auf der Kommandozeile? Wenn Du dich per ssh wohin verbindest, gibst du Benutzername, Hostnamen und privaten Schlüssel jedes mal as Parameter an? Du meldest dich manuell am jump/bastion host an, bevor Du dich auf Dein eigentliches Zielsystem anmeldest?\nDann schau in meiner Session vorbei, in deren Rahmen ich Dir zeige, wie Du Dir Deine Arbeit mit ssh einfacher und effektiver gestalten kannst. Und das alles mit minimalem Aufwand und geringer Vorbereitung. Weiters werfen wir natürlich einen Blick auf best practices und wie ihr euer SSH Setup verbessern könnt.\n\nAm meisten nimmst Du aus dem Talk mit, wenn Du ssh schon mal verwendet, aber ansonsten noch nicht viel darüber nachgedacht hast. SSH EinsteigerInnen sind selbstverständlich auch herzlich willkommen. SSH Profis finden maximal die Witze in diesem Vortrag interessant.","accessed":{"date-parts":[["2022",7,12]]},"author":[{"literal":"Leyrer"}],"citation-key":"leyrerBesserLebenMit2022","event-title":"Gulaschprogrammiernacht 20","issued":{"date-parts":[["2022",5,21]]},"language":"de","title":"Besser leben mit SSH","type":"speech","URL":"https://media.ccc.de/v/gpn20-8-besser-leben-mit-ssh"},
  {"id":"mikehanleyWeUpdatedOur2023","abstract":"At approximately 05:00 UTC on March 24, out of an abundance of caution, we replaced our RSA SSH host key used to secure Git operations for GitHub.com.","accessed":{"date-parts":[["2024",2,18]]},"author":[{"literal":"Mike Hanley"}],"citation-key":"mikehanleyWeUpdatedOur2023","container-title":"GitHub Blog","genre":"Blog","issued":{"date-parts":[["2023",3,23]]},"language":"en","title":"We updated our RSA SSH host key","type":"post-weblog","URL":"https://github.blog/2023-03-23-we-updated-our-rsa-ssh-host-key/"},
    {"id":"liPyTorchDistributedExperiences2020","abstract":"This paper presents the design, implementation, and evaluation of the PyTorch distributed data parallel module. Py-Torch is a widely-adopted scientific computing package used in deep learning research and applications. Recent advances in deep learning argue for the value of large datasets and large models, which necessitates the ability to scale out model training to more computational resources. Data parallelism has emerged as a popular solution for distributed training thanks to its straightforward principle and broad applicability. In general, the technique of distributed data parallelism replicates the model on every computational resource to generate gradients independently and then communicates those gradients at each iteration to keep model replicas consistent. Despite the conceptual simplicity of the technique, the subtle dependencies between computation and communication make it non-trivial to optimize the distributed training efficiency. As of v1.5, PyTorch natively provides several techniques to accelerate distributed data parallel, including bucketing gradients, overlapping computation with communication, and skipping gradient synchronization. Evaluations show that, when configured appropriately, the PyTorch distributed data parallel module attains near-linear scalability using 256 GPUs.","accessed":{"date-parts":[["2023",6,19]]},"author":[{"family":"Li","given":"Shen"},{"family":"Zhao","given":"Yanli"},{"family":"Varma","given":"Rohan"},{"family":"Salpekar","given":"Omkar"},{"family":"Noordhuis","given":"Pieter"},{"family":"Li","given":"Teng"},{"family":"Paszke","given":"Adam"},{"family":"Smith","given":"Jeff"},{"family":"Vaughan","given":"Brian"},{"family":"Damania","given":"Pritam"},{"family":"Chintala","given":"Soumith"}],"citation-key":"liPyTorchDistributedExperiences2020","container-title":"Proceedings of the VLDB Endowment","container-title-short":"Proc. VLDB Endow.","DOI":"10.14778/3415478.3415530","ISSN":"2150-8097","issue":"12","issued":{"date-parts":[["2020",8]]},"language":"en","page":"3005-3018","source":"DOI.org (Crossref)","title":"PyTorch Distributed: Experiences on Accelerating Data Parallel Training","title-short":"PyTorch distributed","type":"article-journal","URL":"https://dl.acm.org/doi/10.14778/3415478.3415530","volume":"13"},
  {"id":"liPyTorchDistributedOverview2024","abstract":"This is the overview page for the torch.distributed package. The goal of this page is to categorize documents into different topics and briefly describe each of them. If this is your first time building distributed training applications using PyTorch, it is recommended to use this document to navigate to the technology that can best serve your use case.","accessed":{"date-parts":[["2024",3,8]]},"author":[{"family":"Li","given":"Shen"}],"citation-key":"liPyTorchDistributedOverview2024","issued":{"date-parts":[["2024",1,23]]},"language":"en","title":"PyTorch Distributed Overview","type":"document","URL":"https://pytorch.org/tutorials/beginner/dist_overview.html"},
  {"id":"nielsenIntroductionHPCMPI2016","accessed":{"date-parts":[["2024",1,28]]},"author":[{"family":"Nielsen","given":"Frank"}],"citation-key":"nielsenIntroductionHPCMPI2016","collection-title":"Undergraduate Topics in Computer Science","DOI":"10.1007/978-3-319-21903-5","event-place":"Cham","ISBN":"978-3-319-21902-8 978-3-319-21903-5","issued":{"date-parts":[["2016"]]},"publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Introduction to HPC with MPI for Data Science","type":"book","URL":"http://link.springer.com/10.1007/978-3-319-21903-5"},
  {"id":"paszkePyTorchImperativeStyle2019","abstract":"Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.","accessed":{"date-parts":[["2023",3,31]]},"author":[{"family":"Paszke","given":"Adam"},{"family":"Gross","given":"Sam"},{"family":"Massa","given":"Francisco"},{"family":"Lerer","given":"Adam"},{"family":"Bradbury","given":"James"},{"family":"Chanan","given":"Gregory"},{"family":"Killeen","given":"Trevor"},{"family":"Lin","given":"Zeming"},{"family":"Gimelshein","given":"Natalia"},{"family":"Antiga","given":"Luca"},{"family":"Desmaison","given":"Alban"},{"family":"Köpf","given":"Andreas"},{"family":"Yang","given":"Edward"},{"family":"DeVito","given":"Zach"},{"family":"Raison","given":"Martin"},{"family":"Tejani","given":"Alykhan"},{"family":"Chilamkurthy","given":"Sasank"},{"family":"Steiner","given":"Benoit"},{"family":"Fang","given":"Lu"},{"family":"Bai","given":"Junjie"},{"family":"Chintala","given":"Soumith"}],"citation-key":"paszkePyTorchImperativeStyle2019","DOI":"10.48550/arXiv.1912.01703","issued":{"date-parts":[["2019",12,3]]},"number":"arXiv:1912.01703","publisher":"arXiv","source":"arXiv.org","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library","title-short":"PyTorch","type":"article","URL":"http://arxiv.org/abs/1912.01703"},
  {"id":"zhaoPyTorchFSDPExperiences2023","abstract":"It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.","accessed":{"date-parts":[["2023",4,28]]},"author":[{"family":"Zhao","given":"Yanli"},{"family":"Gu","given":"Andrew"},{"family":"Varma","given":"Rohan"},{"family":"Luo","given":"Liang"},{"family":"Huang","given":"Chien-Chin"},{"family":"Xu","given":"Min"},{"family":"Wright","given":"Less"},{"family":"Shojanazeri","given":"Hamid"},{"family":"Ott","given":"Myle"},{"family":"Shleifer","given":"Sam"},{"family":"Desmaison","given":"Alban"},{"family":"Balioglu","given":"Can"},{"family":"Nguyen","given":"Bernard"},{"family":"Chauhan","given":"Geeta"},{"family":"Hao","given":"Yuchen"},{"family":"Li","given":"Shen"}],"citation-key":"zhaoPyTorchFSDPExperiences2023","DOI":"10.48550/arXiv.2304.11277","issued":{"date-parts":[["2023",4,21]]},"number":"arXiv:2304.11277","publisher":"arXiv","source":"arXiv.org","title":"PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel","title-short":"PyTorch FSDP","type":"article","URL":"http://arxiv.org/abs/2304.11277"}
]
